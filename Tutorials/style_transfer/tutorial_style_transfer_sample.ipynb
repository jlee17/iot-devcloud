{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebook Style Transfer Sample Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface: How to Use this Jupyter Notebook Tutorial\n",
    "Below is a sequence of boxes referred to as \"cells\".  Each cell will contain text, like this one, or Python code that may be executed as part of this tutorial.  As you go through this turoial, please note the following:\n",
    "\n",
    "### Running the Tutorial\n",
    "You can always use either the \"Run\" button at the top or **Shift+Enter** to execute a selected cell, starting with this one, and then automatically move to the next cell.\n",
    "\n",
    "   **Note**: If you happen to press just **Enter**, you will enter the editing mode for the cell.  To exit and continue, use **Shift+Enter**.  \n",
    "   \n",
    "Unless stated otherwise, the cells containing code within this tutorial **MUST** be executed in sequence.\n",
    "\n",
    "You may save the tutorial at any time which will save the output, but not state.  Saved Jupyter Notebooks will save sequence numbers which may make a cell appear executed when it has not been for the new session.  Because state is not saved, re-opening or restarting a Jupyter Notebook will required re-executing all the executable steps starting from the beginning.\n",
    "\n",
    "If for any reason you need to restart the tutorial from the beginning, you may reset the state of the Jupyter Notebook and clear all output by using the menu at the top by selecting **Kernel->\"Restart and Clear Output\"**\n",
    "\n",
    "### Cells Containing Executable Code\n",
    "Executable cells will have \"In [n]:\" to the left of the cell:\n",
    "- If 'n' is blank (no number), it means that the cell has not yet been executed.  \n",
    "- If 'n' is '*', it means that the cell is currently executing.  \n",
    "- Once a cell is done executing, 'n' will appear as a number incrementing with each cell execution to indicate where in the sequence the cell has been executed.  Any output (e.g. print()'s) from the code will appear below the cell.\n",
    "    - Note: If you need to stop a cell during execution, for example during a long video, you can use the \"Stop\" button at the top (square to the right of the \"Run\" button).  After stopping the cell, you may re-execute it if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "Before going through this tutorial, please be sure that:\n",
    "- All files from the .zip file containing the tutorial are present and in the same directory.  The required files are:\n",
    "    - tutorial_style_sample.ipynb - This Jupyter Notebook\n",
    "    - nst_vgg19/nst_vgg19-symbol.bin and nst_vgg19/nst_vgg19-symbol.xml - The IR files for the inference model created using Model Optimizer\n",
    "    - tubingen.jpg - test image\n",
    "    - starry_night.jpg - test image\n",
    "    - libcpu_extension.so - pre-compiled CPU extension library\n",
    "    - doc_*.png - images used in the documentation\n",
    "- Optional: URL to image or user's video to run inference on\n",
    "\n",
    "**Note:** It is assumed that the server this tutorial is being run on has Jupyter Notebook, OpenVINO toolkit, and other required libraries already installed.  If you download or copy to a new server, the tutorial may not run.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The purpose of this tutorial is to examine a sample application that was created using the [Intel® Distribution of Open Visual Inference & Neural Network Optimization (OpenVINO™) toolkit](https://software.intel.com/openvino-toolkit).  This tutorial will go step-by-step through the necessary steps to demonstrate style transfer on images and video .  Style transfer, where the style of one image is transferred to another as if recreated using the same style, is performed using a pre-trained network and running it using the OpenVINO™ toolkit Inference Engine.  Inference will be executed using the same CPU(s) running this Jupyter Notebook.\n",
    "\n",
    "The pre-trained model to be used for object detection is the [\"VGG19\"](https://github.com/dmlc/web-data/tree/master/mxnet/neural-style) which has already been converted to the necessary Intermediate Representation (IR) files needed by the Inference Engine (Conversion is not covered here, please see the [Intel® Distribution of OpenVINO™ toolkit](https://software.intel.com/en-us/openvino-toolkit) documentation, specifically the **Converting a Style Transfer Model from MXNet** instructions for more details).  The model used will re-style input images to a an abstracted \"brush stroked\" version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Concepts\n",
    "Before going into the samples in the tutorial steps, first we will go over some key concepts that will be covered in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINO™ Toolkit Overview and Terminology \n",
    "\n",
    "Let us begin with a brief overview of the OpenVINO™ toolkit and what this tutorial will be covering.  The OpenVINO™ toolkit enables the quick deployment of convolutional neural networks (CNN) for heterogeneous execution on Intel® hardware while maximizing performance. This is done using the Intel® Deep Learning Deployment Toolkit (Intel® DL Deployment Toolkit) included within the OpenVINO™ toolkit with its main components shown below.\n",
    "\n",
    "![image alt text](./doc_openvino_overview_image.png)\n",
    "\n",
    "The basic flow is:\n",
    "\n",
    "1. Use a tool, such as Caffe, to create and train a CNN inference model\n",
    "\n",
    "2. Run the created model through Model Optimizer to produce an optimized Intermediate Representation (IR) stored in files (.bin and .xml) for use with the Inference Engine\n",
    "\n",
    "3. The User Application then loads and runs models on devices using the Inference Engine and the IR files  \n",
    "\n",
    "This tutorial will focus on the last step, the User Application and using the Inference Engine to run a model on CPU.\n",
    "\n",
    "### Using the Inference Engine\n",
    "\n",
    "Below is a more detailed view of the User Application and Inference Engine:\n",
    "\n",
    "![image alt text](./doc_inference_engine_image.png)\n",
    "\n",
    "The Inference Engine includes a plugin library for each supported device that has been optimized for the Intel® hardware device CPU, GPU, and Myriad.  From here, we will use the terms \"device\" and “plugin” with the assumption that one infers the other (e.g. CPU device infers the CPU plugin and vice versa).  As part of loading the model, the User Application tells the Inference Engine which device to target which in turn loads the associated plugin library to later run on the associated device. The Inference Engine uses “blobs” for all data exchanges, basically arrays in memory arranged according the input and output data of the model.\n",
    "\n",
    "#### Inference Engine API Integration Flow\n",
    "\n",
    "Using the Inference Engine API follows the basic steps outlined briefly below.  The API objects and functions will be seen later in the sample code.\n",
    "\n",
    "1. Load the plugin\n",
    "\n",
    "2. Read the model IR\n",
    "\n",
    "3. Load the model into the plugin\n",
    "\n",
    "6. Prepare the input\n",
    "\n",
    "7. Run Inference\n",
    "\n",
    "8. Process the output\n",
    "\n",
    "More details on the Inference Engine can be found in the [Inference Engine Development Guide](https://software.intel.com/inference-engine-devguide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Preprocessing\n",
    "\n",
    "Often, the dimensions of the input data does not match the required dimensions of the input data for the inference model.  A common example is an input video frame.  Before the image may be input to the inference model, the input must be preprocessed to match the required dimensions for the inference model as well as channels (i.e. colors) and batch size (number of images present).  The basic step performed is to resize the frame from the source dimensions to match the required dimensions of the inference model’s input, reorganizing any dimensions as needed.\n",
    "\n",
    "This tutorial and the many samples in the OpenVINO™ toolkit use OpenCV to perform resizing of input data.  The basic steps performed using OpenCV are:\n",
    "\n",
    "1.  Resize image dimensions form image to model's input W x H:\n",
    "    frame = cv2.resize(image, (w, h))\n",
    "   \n",
    "2. Change data layout from (H x W x C) to (C x H x W)\n",
    "    frame = frame.transpose((2, 0, 1))  \n",
    "\n",
    "3. Reshape to match input dimensions\n",
    "    frame = frame.reshape((n, c, h, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Application\n",
    "Now we will begin going through the sample application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Python Modules\n",
    "Here we begin by importing all the Python modules that will be used by the sample code:\n",
    "- [os](https://docs.python.org/3/library/os.html#module-os) - Operating system specific module (used for file name parsing)\n",
    "- [cv2](https://docs.opencv.org/trunk/) - OpenCV module\n",
    "- [time](https://docs.python.org/3/library/time.html#module-time) - time tracking module (used for measuring execution time)\n",
    "- [numpy](http://www.numpy.org/) - n-dimensional array manipulation\n",
    "- [openvino.inference_engine](https://software.intel.com/en-us/articles/OpenVINO-InferEngine) - import the IENetwork and IEPlugin objects\n",
    "- [matplotlib](https://matplotlib.org/) - import pyplot used for displaying output images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "from openvino.inference_engine import IENetwork, IEPlugin\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "print(\"Imported Python modules.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Parameters\n",
    "Here we will create and set the following configuration parameters used by the sample:  \n",
    "- *model_xml* - Path to the .xml IR file of the trained model to use for inference\n",
    "- *model_bin* - Path to the .bin IR file of the trained model to use for inference (derived from *model_xml*)\n",
    "- *input_path* - Path to input image\n",
    "- *cpu_extension_path* - Path to a shared library with CPU extension kernels for custom layers not already included in plugin\n",
    "- *device* - Specify the target device to infer on,  CPU, GPU, FPGA, or MYRIAD is acceptable, however the device must be present.  For this tutorial we use \"CPU\" which is known to be present.\n",
    "- *mean_val_r* - Mean value of red chanel for mean value addition in postprocessing \n",
    "- *mean_val_g* - Mean value of green chanel for mean value addition in postprocessing\n",
    "- *mean_val_b* - Mean value of blue chanel for mean value addition in postprocessing\n",
    "\n",
    "We will set all parameters here only once except for *input_path* which we will change later to point to different images and video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model IR files\n",
    "model_xml = \"./nst_vgg19/nst_vgg19-symbol.xml\"\n",
    "model_bin = \"./nst_vgg19/nst_vgg19-symbol.bin\"\n",
    "\n",
    "# input image file\n",
    "input_path = \"tubingen.jpg\"\n",
    "\n",
    "# CPU extension library to use\n",
    "cpu_extension_path = \"libcpu_extension.so\"\n",
    "\n",
    "# device to use\n",
    "device = \"CPU\"\n",
    "\n",
    "# RGB mean values to add to results\n",
    "mean_val_r = 123.68\n",
    "mean_val_g = 116.779\n",
    "mean_val_b = 103.939\n",
    "\n",
    "print(\"Configuration parameters settings:\"\n",
    "     \"\\n\\tmodel_xml=\", model_xml,\n",
    "      \"\\n\\tmodel_bin=\", model_bin,\n",
    "      \"\\n\\tinput_path=\", input_path,\n",
    "      \"\\n\\tcpu_extension_path=\", cpu_extension_path, \n",
    "      \"\\n\\tdevice=\", device, \n",
    "      \"\\n\\tmean_val_r=\", mean_val_r,\n",
    "      \"\\n\\tmean_val_g=\", mean_val_g,\n",
    "      \"\\n\\tmean_val_b=\", mean_val_b )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Plugin for Device\n",
    "Here we create a plugin object for the specified device using IEPlugin().  \n",
    "If the plugin is for a CPU device, and the *cpu_extensions_path* variable has been set, we load the extensions library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plugin for device\n",
    "plugin = IEPlugin(device=device)\n",
    "print(\"A plugin object has been created for device [\", plugin.device, \"]\\n\")\n",
    "\n",
    "# if the device is CPU and a path to an extension library is set, load the extension library \n",
    "if cpu_extension_path and 'CPU' in device:\n",
    "    plugin.add_cpu_extension(cpu_extension_path)\n",
    "    print(\"CPU extension [\", cpu_extension_path, \"] has been loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Network from Model IR files\n",
    "Here we create a *IENetwork* object and load the model's IR files into it.  After loading the model, we check to make sure that all the model's layers are supported by the plugin we will use.  We also check to make sure that the model's input and output are as expected for later when we run inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load network from IR files\n",
    "net = IENetwork.from_ir(model=model_xml, weights=model_bin)\n",
    "print(\"Loaded model IR files [\",model_bin,\"] and [\", model_xml, \"]\\n\")\n",
    "\n",
    "# check to make sure that the plugin has support for all layers in the loaded model\n",
    "supported_layers = plugin.get_supported_layers(net)\n",
    "not_supported_layers = [l for l in net.layers.keys() if l not in supported_layers]\n",
    "if len(not_supported_layers) != 0:\n",
    "    print(\"ERROR: Following layers are not supported by the plugin for specified device {}:\\n {}\".\n",
    "             format(plugin.device, ', '.join(not_supported_layers)))\n",
    "    if not cpu_extension_path:\n",
    "        print(\"       Please try specifying the cpu extensions library path by setting the 'cpu_extension_path' variable\")\n",
    "    assert 0 == 1, \"ERROR: Missing support for all layers in th emodel, cannot continue.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model into the Device Plugin\n",
    "Here we load the model network into the plugin so that we may run inference.  *exec_net* will be used later to actually run inference.  After loading, we store the names of the input (*input_blob*) and output (*output_blob*) blobs to use when accessing the input and output blobs of the model.  Lastly, we store the model's input dimensions into the following variables:\n",
    "- *n* = input batch size\n",
    "- *c* = number of input channels (here 1 channel per color R,G, and B)\n",
    "- *h* = input height\n",
    "- *w* = input width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model into the plugin\n",
    "exec_net = plugin.load(network=net, num_requests=2)\n",
    "\n",
    "# store name of input and output blobs\n",
    "input_blob = next(iter(net.inputs))\n",
    "output_blob = next(iter(net.outputs))\n",
    "\n",
    "# read the input's dimensions: n=batch size, c=number of channels, h=height, w=width\n",
    "n, c, h, w = net.inputs[input_blob].shape\n",
    "print(\"Loaded model into plugin.  Model input dimensions: n=\",n,\", c=\",c,\", h=\",h,\", w=\",w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Input Image\n",
    "Here we read and then prepare the input image by resizing and re-arranging its dimensions according to the model's input dimensions.  We define functions the *loadInputImage()* and *resizeInputImage()* for the operations so that we may reuse them again later in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to load an input image\n",
    "def loadInputImage(input_path, verbose = True):\n",
    "    # globals to store input width and height\n",
    "    global input_w, input_h\n",
    "    \n",
    "    # use OpenCV to load the input image\n",
    "    cap = cv2.VideoCapture(input_path) \n",
    "    \n",
    "    # store input width and height\n",
    "    input_w = cap.get(3)\n",
    "    input_h = cap.get(4)\n",
    "    if verbose: print(\"Loaded input image [\",input_path,\"], resolution=\", input_w, \"w x \",input_h,\"h\")\n",
    "\n",
    "    # load the input image\n",
    "    ret, image = cap.read()\n",
    "    del cap\n",
    "    return image\n",
    "\n",
    "# define function for resizing input image\n",
    "def resizeInputImage(image, verbose = True):\n",
    "    # resize image dimensions form image to model's input w x h\n",
    "    in_frame = cv2.resize(image, (w, h))\n",
    "    # Change data layout from HWC to CHW\n",
    "    in_frame = in_frame.transpose((2, 0, 1))  \n",
    "    # reshape to input dimensions\n",
    "    in_frame = in_frame.reshape((n, c, h, w))\n",
    "    if verbose: print(\"Resized input image from {} to {}\".format(image.shape[:-1], (h, w)))\n",
    "    return in_frame\n",
    "\n",
    "# load image\n",
    "image = loadInputImage(input_path)\n",
    "\n",
    "# resize the input image\n",
    "in_frame = resizeInputImage(image)\n",
    "\n",
    "# display input image\n",
    "print(\"Input image:\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference\n",
    "Now that we have the input image in the correct format for the model, we now run inference on the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save start time\n",
    "inf_start = time.time()\n",
    "\n",
    "# run inference\n",
    "res = exec_net.infer(inputs={input_blob: in_frame})   \n",
    "\n",
    "# calculate time from start until now\n",
    "inf_time = time.time() - inf_start\n",
    "print(\"Inference complete, run time: {:.3f} ms\".format(inf_time * 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process and Display Results\n",
    "Now we process the inference results by sorting all the possible classes by the probability assigned during inference, then selecting and displaying the top *report_top_n* items.  We define the function *processAndDisplayResults()* so that we may use it again later in the tutorial to process results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to process inference results\n",
    "def processResults(res):\n",
    "    # get output\n",
    "    result = res[output_blob][0]\n",
    "    \n",
    "    # Change layout from CxHxW to HxWxC \n",
    "    result = np.swapaxes(result, 0, 2)\n",
    "    result = np.swapaxes(result, 0, 1)\n",
    "    \n",
    "    # add RGB mean values to \n",
    "    result = result[::] + (mean_val_r, mean_val_g, mean_val_b)\n",
    "    \n",
    "    # Clip RGB values to [0, 255] range\n",
    "    result[result < 0] = 0\n",
    "    result[result > 255] = 255\n",
    "\n",
    "    # Matplotlub expects normilized image with pixel RGB values in range [0,1].\n",
    "    result = result / 255\n",
    "    return result\n",
    "    \n",
    "# create function to process and display inference results\n",
    "def processAndDisplayResults(res, orig_input_image, orig_input_path, verbose = True):\n",
    "    # display original input image\n",
    "    plt.figure()\n",
    "    plt.axis(\"off\")\n",
    "    im_to_show = cv2.cvtColor(orig_input_image, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(im_to_show)\n",
    "    \n",
    "    # get output\n",
    "    result = processResults(res)\n",
    "       \n",
    "    # Show styled image\n",
    "    if verbose: print(\"Results for input image: {}\".format(orig_input_path))\n",
    "    plt.figure()\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(result)\n",
    "\n",
    "processAndDisplayResults(res, image, input_path)\n",
    "print(\"Processed and displayed inference output results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #1: Run a Different Image\n",
    "Now that we have seen all the steps, let us run them again on a different image.  We also define *inferImage()* to combine the input processing, inference, and processing and displaying results so that we may use it again later in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to prepare input and run inference\n",
    "def inferImage(image, verbose = True):\n",
    "    # prepare input\n",
    "    in_frame = resizeInputImage(image, verbose)\n",
    "\n",
    "    # run inference\n",
    "    res = exec_net.infer(inputs={input_blob: in_frame})   \n",
    "    return res\n",
    "\n",
    "# define function to prepare input, run inference, and process inference results\n",
    "def inferAndDisplayImage(image, verbose = True):\n",
    "\n",
    "    # run inference\n",
    "    res = inferImage(image)\n",
    "\n",
    "    # process inference results \n",
    "    processAndDisplayResults(res, image, input_path, verbose)\n",
    "\n",
    "# set path to differnt input image\n",
    "input_path=\"starry_night.jpg\"\n",
    "\n",
    "# load input image\n",
    "image = loadInputImage(input_path)\n",
    "\n",
    "# infer image and display results\n",
    "inferAndDisplayImage(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #2: (Optional) Run Your Own Image\n",
    "Here you may run any image you would like by setting the *input_path* variable which may be set to a local file or URL.  A sample URL is provided as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_path may be set to a local file or URL\n",
    "input_path = \"https://github.com/dmlc/web-data/raw/master/mxnet/neural-style/input/IMG_4343.jpg\"\n",
    "\n",
    "# load input image\n",
    "image = loadInputImage(input_path)\n",
    "\n",
    "# infer image and display results\n",
    "inferAndDisplayImage(image, input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #3: Running Inference on Video\n",
    "We have seen how to run individual images, now how do we do video?  To run inference on video is much the same as for a single image except that a loop is necessary to process all the frames in the video.  In the code below, we use the same method of loading a video as we had for an image, but now include the while-loop to keep reading images until *cap.isOpened()* returns false or *cap.read()* sets *ret* to false:\n",
    "\n",
    "while cap.isOpened():\n",
    "    # read video frame\n",
    "    ret, im = cap.read()\n",
    "   \n",
    "    # break if no more video frames\n",
    "    if not ret:\n",
    "        break  \n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close and then re-import matplotlib to be able to update output images for video\n",
    "plt.close()\n",
    "%matplotlib notebook\n",
    "from matplotlib import pyplot as plt\n",
    "# create figures and disable axis display\n",
    "plt.figure(1)\n",
    "plt.axis(\"off\")\n",
    "plt.figure(2)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# input_path may be set to local file or URL \n",
    "input_path=\"/opt/intel/computer_vision_sdk/deployment_tools/inference_engine/samples/end2end_video_analytics/test_content/video/cars_768x768.h264\"\n",
    "#input_path=\"https://github.com/intel-iot-devkit/sample-videos/raw/master/worker-zone-detection.mp4\"\n",
    "\n",
    "print(\"Loading video [\",input_path,\"]\")\n",
    "# use OpenCV to load the input image\n",
    "cap = cv2.VideoCapture(input_path) \n",
    "\n",
    "# store input width and height\n",
    "input_w = cap.get(3)\n",
    "input_h = cap.get(4)\n",
    "print(\"Loaded input video [\",input_path,\"], resolution=\", input_w, \"w x \",input_h,\"h\")\n",
    "\n",
    "# track frame count and set maximum\n",
    "frame_num = 0\n",
    "max_frame_num = 6\n",
    "skip_num_frames = 200\n",
    "last_frame_num = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "if last_frame_num < 1: last_frame_num = \"unknown\"\n",
    "\n",
    "while cap.isOpened():\n",
    "    # read video frame\n",
    "    ret, image = cap.read()\n",
    "   \n",
    "    # break if no more video frames\n",
    "    if not ret:\n",
    "        break  \n",
    "    \n",
    "    frame_num += 1\n",
    "\n",
    "    # skip skip_num_frames of frames, then infer max_frame_num frames from there\n",
    "    if frame_num > skip_num_frames: \n",
    "        # infer image\n",
    "        res = inferImage(image, False)\n",
    "\n",
    "        # show original image then force re-draw to show new image\n",
    "        plt.figure(1)\n",
    "        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        plt.gcf().canvas.draw()\n",
    "\n",
    "        # process results\n",
    "        result = processResults(res)        \n",
    "\n",
    "        # display results\n",
    "        plt.figure(2)\n",
    "        plt.imshow(result)\n",
    "        plt.gcf().canvas.draw()\n",
    "\n",
    "   \n",
    "    # display current frame number\n",
    "    print(\"Frame #:\", frame_num, \"/\", last_frame_num, end=\"\\r\")\n",
    "    \n",
    "    # limit number of frames, video can be slow and gets slower the more frames that are processed\n",
    "    if frame_num >= (max_frame_num + skip_num_frames): \n",
    "        print(\"\\nStopping at frame #\", frame_num)\n",
    "        break\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #4: (Optional) Run Your Own Video\n",
    "If you would like to see inference run on your own video, you may do so by first setting *input_path* to a local file or URL and then re-executing the cell above.  For example, you could use this video:\n",
    "https://github.com/intel-iot-devkit/sample-videos/raw/master/worker-zone-detection.mp4 by replacing the *input_path=\"...\"* line above with the line:\n",
    "\n",
    "input_path=\"https://github.com/intel-iot-devkit/sample-videos/raw/master/worker-zone-detection.mp4\"\n",
    "\n",
    "You can control which frame to start from by setting *skip_num_frames* which will skip that many frames.\n",
    "YOu can also control how many frames to show by setting *max_frame_num*.\n",
    "\n",
    "**Note:** There are more videos available to choose from at: https://github.com/intel-iot-devkit/sample-videos/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exit: Free Resources\n",
    "Now that we are done running the sample, we clean up by deleting objects before exiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del net\n",
    "del exec_net\n",
    "del plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Tutorial - Next Steps\n",
    "\n",
    "### [More Jupyter Notebook Tutorials](https://access.colfaxresearch.com/?p=experience)\n",
    "### [Intel® Distribution of OpenVINO™ toolkit Main Page](https://software.intel.com/openvino-toolkit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
