{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='red'>  Important: </font> Before proceeding, run the following cell to check for code updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qarpo.catalog import DemoCatalog\n",
    "import os\n",
    "status = DemoCatalog(os.getcwd(), \"Demo\").ShowRepositoryControls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection Demo: Safety Gear Detection\n",
    "\n",
    "\n",
    "\n",
    "This is a sample reference implementation to showcase object detection (safety gear in this case) with single-shot detection (SSD) and Async API.\n",
    "Async API improves the overall frame-rate of the application by not waiting for the inference to complete, continuing to do things on the host while inference accelerator is busy. \n",
    "Specifically, this code demonstrates two parallel inference requests by processing the current frame while the next input frame is being captured. This essentially hides the latency of frame capture.\n",
    "\n",
    "## Overview of how it works\n",
    "At start-up the sample application reads the equivalent of command line arguments and loads a network and image from the video input to the Inference Engine (IE) plugin. \n",
    "A job is submitted to an edge compute node with a hardware accelerator such as Intel® HD Graphics GPU, Intel® Movidius™ Vision Processing Unit(VPU) or Intel® Arria® 10 FPGA.\n",
    "After the inference is completed, the output videos are appropriately stored in the /results directory, which can then be viewed within the Jupyter Notebook instance.\n",
    "\n",
    "## Demonstration objectives\n",
    "* Video as input is supported using **OpenCV**\n",
    "* Inference performed on edge hardware (rather than on the development node hosting this Jupyter Notebook)\n",
    "* **OpenCV** provides the bounding boxes, labels and other information\n",
    "* Visualization of the resulting bounding boxes\n",
    "* Demonstrate the Async API in action\n",
    "\n",
    "\n",
    "## Step 1: Set Up\n",
    "\n",
    "### 1.1: Import dependencies\n",
    "\n",
    "Run the below cell to import Python dependencies needed for displaying the results in this notebook\n",
    "(tip: select the cell and use **Ctrl+enter** to run the cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from qarpo.demoutils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2  (Optional step): Original video without inference\n",
    "\n",
    "If you are curious to see the input video, run the following cell to view the original video stream used for inference and object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -sf /data/reference-sample-data/safety-gear-detection/Safety_Full_Hat_and_Vest.mp4\n",
    "videoHTML('Workers video', ['Safety_Full_Hat_and_Vest.mp4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Using Intel® Distribution of OpenVINO™ toolkit\n",
    "\n",
    "First, let's try running inference on a single image to see how Intel® Distribution of OpenVINO™ toolkit works.\n",
    "We will be using Intel® Distribution of OpenVINO™ toolkit's Inference Engine (IE) to detect safety gear (hat, vest, etc).\n",
    "There are five steps involved in this task:\n",
    "\n",
    "1. Create an Intermediate Representation (IR) Model using the Model Optimizer from Intel\n",
    "2. Choose a device and create IEPlugin for the device\n",
    "3. Read the IRModel using IENetwork\n",
    "4. Load the IENetwork into the Plugin\n",
    "5. Run inference.\n",
    "\n",
    "### 2.1 Creating IR Model\n",
    "\n",
    "The Model Optimizer from Intel creates Intermediate Representation (IR) models that are optimized for different end-point target devices.\n",
    "These models can be created from existsing DNN models from popular frameworks (e.g. Caffe*, TF) using the Model Optimizer from Intel. \n",
    "\n",
    "The Intel® Distribution of OpenVINO™ toolkit includes an utility script `model_downloader.py` that you can use to download some common modes. Run the following cell to see the models available through `model_downloader.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --print_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The '!' is a special Jupyter Notebook command that allows you to run shell commands as if you are in command line. So, the above command will work straight out of the box on in a terminal (with '!' removed).\n",
    "\n",
    "To download a specific model, run the model downloader while specifing the input arguments as follows:\n",
    "* --name : name of the model you want to download. It should be one of the models listed in the previous cell\n",
    "* -o : output directory. If this directory does not exist, it will be created for you.\n",
    "\n",
    "There are more arguments to this script and you can get the full list using the `-h` option.\n",
    "\n",
    "For example, to download the **mobilenet-ssd** model into **raw_models** directory, run the following cell.\n",
    "\n",
    "**Note**: This model downloaded will not be used for the demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name mobilenet-ssd -o raw_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For this demo**, we will use an existing Caffe model specially trained for detecting people, hardhats and safety vests.\n",
    "Let's convert the model using the model optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
    "--input_model /data/reference-sample-data/safety-gear-detection/worker_safety_mobilenet.caffemodel \\\n",
    "--model_name mobilenet-ssd \\\n",
    "--data_type FP32 \\\n",
    "-o models/mobilenet-ssd/FP32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The above line is a single command line input, which spans 4 lines thanks to the backslash '\\\\', which is a line continuation character in Bash.\n",
    "\n",
    "Here, the arguments are:\n",
    "* --input-model : the original model\n",
    "* --data_type : Data type to use. One of {FP32, FP16, half, float}\n",
    "* -o : output directory\n",
    "\n",
    "This script also supports `-h` that will you can get the full list of arguments.\n",
    "\n",
    "With the `-o` option set as above, this command will write the output to the directory `models/mobilenet-ssd/FP32`\n",
    "\n",
    "There are two files produced:\n",
    "```\n",
    "models/mobilenet-ssd/FP32/mobilenet-ssd.xml\n",
    "models/mobilenet-ssd/FP32/mobilenet-ssd.bin\n",
    "```\n",
    "These will be used later in the exercise.\n",
    "\n",
    "We will also be needing the FP16 version of the model for the calculations on the MYRIAD architecture. Run the following cell to create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
    "--input_model /data/reference-sample-data/safety-gear-detection/worker_safety_mobilenet.caffemodel \\\n",
    "--model_name mobilenet-ssd \\\n",
    "--data_type FP16 \\\n",
    "-o models/mobilenet-ssd/FP16 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Choosing Device\n",
    "\n",
    "Now we must select the device used for inferencing. This is done by loading the appropriate plugin to initialize the specified device and load the extensions library (if specified) provided in the extension/ folder for the device.\n",
    "\n",
    "\n",
    "The following cell constructs **`IEPlugin`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.inference_engine import IEPlugin\n",
    "\n",
    "def createPlugin(device, extension_list):\n",
    "    # Plugin initialization for specified device. We will be targeting CPU initially.\n",
    "    plugin = IEPlugin(device=device)\n",
    "\n",
    "    # Loading additional exension libraries for the CPU\n",
    "    for extension in extension_list:\n",
    "        plugin.add_cpu_extension('/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_avx2.so')\n",
    "    \n",
    "    return plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "Currently, five types of plugins are supported: CPU, GPU, MYRIAD, HDDL and HETERO:FPGA,CPU. CPU plugin may require additional extensions to improve performance, add_cpu_extension function is used to load these additional extensions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2.3 Read the IR (Intermediate Representation) model\n",
    "\n",
    "We can import optimized models (weights) from step 1.1 into our neural network using **`IENetwork`**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.inference_engine import IENetwork\n",
    "\n",
    "def createNetwork(model_xml, model_bin, plugin):\n",
    "    # Importing network weights from IR models.\n",
    "    net = IENetwork(model=model_xml, weights=model_bin)\n",
    "    \n",
    "    # Some layers in IR models may be unsupported by some plugins. \n",
    "    if \"CPU\" in plugin.device:\n",
    "        supported_layers = plugin.get_supported_layers(net)\n",
    "        not_supported_layers = [l for l in net.layers.keys() if l not in supported_layers]\n",
    "        if len(not_supported_layers) != 0:\n",
    "            print(\"Following layers are not supported by the plugin for specified device {}:\\n {}\".\n",
    "                      format(plugin.device, ', '.join(not_supported_layers)))\n",
    "            print(\"Please try to specify cpu extensions library path in sample's command line parameters \"\n",
    "                  \"using -l or --cpu_extension command line argument\")\n",
    "            return None\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "Some models may be incompatible with some target devices. For example, some types of neural network layers are not supported on the CPU target. \n",
    "\n",
    "### 2.4 Load the network into the plugin\n",
    "\n",
    "Once we have the plugin and the network, we can load the network into the plugin using **`plugin.load`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadNetwork(plugin, net):\n",
    "    # Loading IR model to the plugin.\n",
    "    exec_net = plugin.load(network=net, num_requests=2)\n",
    "    \n",
    "    # Getting the input and outputs of the network\n",
    "    input_blob = next(iter(net.inputs))\n",
    "    out_blob = next(iter(net.outputs))\n",
    "    return exec_net,input_blob,out_blob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Run inference\n",
    "\n",
    "Now we are ready to try running the inference workload using the plugin.\n",
    "First let's load the image using OpenCV.\n",
    "We will also have to do some shape manipulation to convert the image to a format that is compatible with our network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def preprocessImage(img_path, net, input_blob):\n",
    "    # Reading the frame from a jpeg file\n",
    "    frame = cv2.imread(img_path)\n",
    "    \n",
    "    # Reshaping data\n",
    "    n, c, h, w = net.inputs[input_blob].shape\n",
    "    in_frame = cv2.resize(frame, (w, h))\n",
    "    in_frame = in_frame.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n",
    "    return in_frame.reshape((n, c, h, w)),frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the inference, we will be running in **async_mode** by using `start_async` method. \n",
    "With the async_mode, the inference is started in parallel on either a separate thread or device.\n",
    "In other words, `start_async` is non-blocking and the main process is free to do any additional processing needed. \n",
    "In the next section, we will see an implementation of pipelining to mask the latency of loading and modifying images.\n",
    "\n",
    "During asynchronous runs, the different images are tracked by an integer `request_id`. \n",
    "Because we only have one image to process, we will just use 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For labeling the image\n",
    "from out_process import placeBoxes\n",
    "\n",
    "\n",
    "# Request id to keep track of\n",
    "def runInference():\n",
    "    plugin = createPlugin(device='CPU', extension_list=['/data/reference-sample-data/extension/libcpu_extension.so'])\n",
    "    model_xml = \"models/mobilenet-ssd/FP32/mobilenet-ssd.xml\"\n",
    "    model_bin = \"models/mobilenet-ssd/FP32/mobilenet-ssd.bin\"\n",
    "    net = createNetwork(model_xml, model_bin, plugin)\n",
    "    exec_net,input_blob,out_blob = loadNetwork(plugin, net)\n",
    "    in_frame,original_frame = preprocessImage('/data/reference-sample-data/safety-gear-detection/safety-gear-image.jpg', net, input_blob)\n",
    "    \n",
    "    my_request_id=0\n",
    "\n",
    "    # Starting the inference in async mode, which starts the inference in parallel\n",
    "    exec_net.start_async(request_id=my_request_id, inputs={input_blob: in_frame})\n",
    "    # ... You can do additional processing or latency masking while we wait ...\n",
    "\n",
    "    # Blocking wait for a particular request_id\n",
    "    if exec_net.requests[my_request_id].wait(-1) == 0:\n",
    "        # getting the result of the network\n",
    "        res = exec_net.requests[my_request_id].outputs[out_blob]\n",
    "\n",
    "        # Processing the output result and adding labels on the image. Implementation is not shown in the\n",
    "        #  this notebook; you can find it in object_detection_demo_ssd_async.py\n",
    "        prob_threshold = 0.5  # 50% confidence needed for \"detection\"\n",
    "        initial_w = original_frame.shape[1]\n",
    "        initial_h = original_frame.shape[0]\n",
    "        labels = \"/data/reference-sample-data/safety-gear-detection/labels.txt\"\n",
    "        frame = placeBoxes(res, labels, prob_threshold, original_frame, initial_w, initial_h, False, my_request_id, 0)\n",
    "        # We use pyplot because it plays nicer with Jupyter Notebooks\n",
    "        fig = plt.figure(dpi=300)\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), interpolation='none')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"There was an error with the request\")\n",
    "runInference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Inference on a video\n",
    "\n",
    "Now that we know how to run inference on a single frame, let's extend this to multiple frames.\n",
    "This part is already implemented in \n",
    "<a href=\"object_detection_demo_ssd_async.py\">object_detection_demo_ssd_async.py</a>.\n",
    "Most of the code is just an extension of the single frame example, but there are few points of importance that we highlight here.\n",
    "\n",
    "The following lines determine the source of the video. We will use a pre-recorded input video file in this example, we could also use a camera by setting the input argument to 'cam'.\n",
    "```python\n",
    "if args.input == 'cam':\n",
    "        input_stream = 0\n",
    "        out_file_name = 'cam'\n",
    "    else:\n",
    "        input_stream = args.input\n",
    "```\n",
    "We capture frames from the video sample using **OpenCV VideoCapture** API.\n",
    "\n",
    "```python\n",
    "cap = cv2.VideoCapture(input_stream)\n",
    "```\n",
    "\n",
    "Finally, we have a latency masking scheme, where we post-process a frames while another frame is being processed on the inference engine.\n",
    "\n",
    "```python\n",
    "cur_request_id = 0\n",
    "next_request_id = 1\n",
    "\n",
    "while cap.isOpened():\n",
    "   # ... load next frame from cap ...\n",
    "\n",
    "   # start the next frame\n",
    "   exec_net.start_async(request_id=next_request_id, inputs={input_blob: in_frame})\n",
    "\n",
    "   # see if the current frame is ready\n",
    "   if exec_net.requests[cur_request_id].wait(-1) == 0:\n",
    "       # ... post-processing current frame ...\n",
    "    \n",
    "   # swap request ids\n",
    "   cur_request_id, next_request_id = next_request_id, cur_request_id\n",
    "```\n",
    "\n",
    "The python code takes in command line arguments for video, model etc.\n",
    "\n",
    "he Python code takes in command line arguments for video, model etc.\n",
    "\n",
    "**Command line arguments options and how they are interpreted in the application source code**\n",
    "\n",
    "```\n",
    "SAMPLEPATH=\"/data/reference-sample-data\"\n",
    "python3 object_detection_demo_ssd_async.py -m ${SAMPLEPATH}/models/mobilenet-ssd/$3/mobilenet-ssd.xml \\\n",
    "                                           -i $INPUT_FILE \\\n",
    "                                           -o $RESULTS_PATH \\\n",
    "                                           -d $DEVICE \\\n",
    "                                           -nireq $NUM_INFER_REQS \\\n",
    "                                           -ce ${SAMPLEPATH}/extension/libcpu_extension.so\n",
    "```\n",
    "\n",
    "##### The description of the arguments used in the argument parser is the command line executable equivalent.\n",
    "* -m location of the **mobilenet-ssd** pre-trained model which has been pre-processed using the **model optimizer**\n",
    "   There is automated support built in this argument to support both FP32 and FP16 models targeting different hardware\n",
    "   (**Note** we are using mobilenet-ssd in this example. However, OpenVINO's Inference Engine is compatible with other neural network architectures such as AlexNet*, GoogleNet*, SqueezeNet* etc.,)    \n",
    "\n",
    "* -i location of the input video stream (video/cars_1900.mp4)\n",
    "* -o location where the output file with inference needs to be stored. (results/core or results/xeon or results/gpu)\n",
    "* -d Type of Hardware Acceleration (CPU or GPU or MYRIAD or HDDL or FPGA)\n",
    "* -nireq Number of inference requests running in parallel\n",
    "* -ce Absolute path to the shared library and is currently optimized for core/xeon (extension/libcpu_extension.so)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Creating job file\n",
    "\n",
    "All the code up to this point has been run within the Jupyter Notebook instance running on a development node based on an Intel® Xeon® Scalable processor, where the Notebook is allocated a single core. \n",
    "To run inference on the entire video, we need more compute power.\n",
    "We will run the workload on several edge compute nodes represented in the IoT DevCloud. We will send work to the edge compute nodes by submitting the corresponding non-interactive jobs into a queue. For each job, we will specify the type of the edge compute node that must be allocated for the job.\n",
    "\n",
    "The job file is written in Bash, and will be executed directly on the edge compute node.\n",
    "For this example, we have written the job file for you in the notebook.\n",
    "Run the following cell to write this in to the file \"object_detection_job.sh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile object_detection_job.sh\n",
    "\n",
    "ME=`basename $0`\n",
    "\n",
    "# The default path for the job is your home directory, so we change directory to where the files are.\n",
    "cd $PBS_O_WORKDIR\n",
    "\n",
    "# Object detection script writes output to a file inside a directory. We make sure that this directory exists.\n",
    "# The output directory is the first argument of the bash script\n",
    "while getopts 'd:f:i:r:n:?' OPTION; do\n",
    "    case \"$OPTION\" in\n",
    "    d)\n",
    "        DEVICE=$OPTARG\n",
    "        echo \"$ME is using device $OPTARG\"\n",
    "      ;;\n",
    "\n",
    "    f)\n",
    "        FP_MODEL=$OPTARG\n",
    "        echo \"$ME is using floating point model $OPTARG\"\n",
    "      ;;\n",
    "\n",
    "    i)\n",
    "        INPUT_FILE=$OPTARG\n",
    "        echo \"$ME is using input file $OPTARG\"\n",
    "      ;;\n",
    "    r)\n",
    "        RESULTS_BASE=$OPTARG\n",
    "        echo \"$ME is using results base $OPTARG\"\n",
    "      ;;\n",
    "    n)\n",
    "        NUM_INFER_REQS=$OPTARG\n",
    "        echo \"$ME is running $OPTARG inference requests\"\n",
    "      ;;\n",
    "    esac  \n",
    "done\n",
    "\n",
    "NN_MODEL=\"mobilenet-ssd.xml\"\n",
    "RESULTS_PATH=\"${RESULTS_BASE}\"\n",
    "mkdir -p $RESULTS_PATH\n",
    "echo \"$ME is using results path $RESULTS_PATH\"\n",
    "\n",
    "if [ \"$DEVICE\" = \"HETERO:FPGA,CPU\" ]; then\n",
    "    # Environment variables and compilation for edge compute nodes with FPGAs\n",
    "    # export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/opt/altera/aocl-pro-rte/aclrte-linux64/\n",
    "    # Environment variables and compilation for edge compute nodes with FPGAs - Updated for 2019-R3\n",
    "    source /opt/intel/init_openvino.sh\n",
    "    aocl program acl0 /opt/intel/openvino/bitstreams/a10_vision_design_sg1_bitstreams/2019R3_PV_PL1_FP11_ResNet_VGG.aocx\n",
    "fi\n",
    "    \n",
    "# Running the object detection code\n",
    "SAMPLEPATH=$PBS_O_WORKDIR\n",
    "python3 object_detection_demo_ssd_async.py  -m ${SAMPLEPATH}/models/mobilenet-ssd/${FP_MODEL}/${NN_MODEL} \\\n",
    "                                            -i $INPUT_FILE \\\n",
    "                                            -o $RESULTS_PATH \\\n",
    "                                            -d $DEVICE \\\n",
    "                                            -nireq $NUM_INFER_REQS \\\n",
    "                                            -ce /opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_avx2.so \\\n",
    "                                            --labels /data/reference-sample-data/safety-gear-detection/labels.txt\n",
    "\n",
    "g++ -std=c++14 ROI_writer.cpp -o ROI_writer_${PBS_JOBID}  -lopencv_core -lopencv_videoio -lopencv_imgproc -lopencv_highgui  -fopenmp -I/opt/intel/openvino/opencv/include/ -L/opt/intel/openvino/opencv/lib/\n",
    "# Rendering the output video\n",
    "SKIPFRAME=1\n",
    "RESOLUTION=0.5\n",
    "./ROI_writer_${PBS_JOBID} $INPUT_FILE $RESULTS_PATH $SKIPFRAME $RESOLUTION\n",
    "rm -f ROI_writer_${PBS_JOBID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Setup the output video specs:\n",
    "\n",
    "\n",
    "After running the object detection code, we have a list of bounding boxes for the detected objects. This may be the end of the object detection workflow some applications that just need the locations, sizes and types of the detected objects. However, if a human needs to view the output video stream, we may want to produce an output video where the bounding boxes are drawn on top of the detected objects.\n",
    "\n",
    "We treat video rendering as a separate task, which is invoked by `ROI_writer` at the end of our job. If you don't want to spend a long time rendering the video, you can reduce the output video quality using the SKIP_FRAME and RESOLUTION variables:\n",
    "\n",
    "* SKIP_FRAME=1 will write all processed video frames with bounding boxes into the output video (this is the slowest option and it preserves all inference data in the output video stream)\n",
    "* SKIP_FRAME=2 will write only every other frames into the output video \n",
    "* RESOLUTION=1 will produce the output video with the same resolution as the input video (this is the slowest option)\n",
    "* RESOLUTION<1 will reduce the output video desolution (e.g., RESOLUTION=0.5 will set the output video resolution in each dimension to 50% of the input video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Understand how jobs are submitted into the queue\n",
    "\n",
    "Now that we have the job script, we can submit the jobs to edge compute nodes. In the IoT DevCloud, you can do this using the `qsub` command.\n",
    "We can submit object_detection_job to 5 different types of edge compute nodes simultaneously or just one node at at time.\n",
    "\n",
    "There are three options of `qsub` command that we use for this:\n",
    "- `-l` : this option lets us select the number and the type of nodes using `nodes={node_count}:{property}`. \n",
    "- `-F` : this option lets us send arguments to the bash script. \n",
    "- `-N` : this option lets use name the job so that it is easier to distinguish between them.\n",
    "\n",
    "The `-F` flag is used to pass in arguments to the job script.\n",
    "The [object_detection_job.sh](object_detection_job.sh) takes in 4 arguments:\n",
    "1. the path to the directory for the output video and performance stats\n",
    "2. targeted device (e.g. CPU,GPU,MYRIAD, HDDl, HETERO:FPGA,CPU)\n",
    "3. the floating precision to use for inference\n",
    "4. the path to the input video\n",
    "\n",
    "The job scheduler will use the contents of `-F` flag as the argument to the job script.\n",
    "\n",
    "If you are curious to see the available types of nodes on the IoT DevCloud, run the following optional cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pbsnodes | grep compnode | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the properties describe the node, and number on the left is the number of available nodes of that architecture.\n",
    "\n",
    "### 3.3 Job queue submission\n",
    "\n",
    "Each of the 4 cells below will submit a job to different edge compute nodes.\n",
    "The output of the cell is the `JobID` of your job, which you can use to track progress of a job.\n",
    "\n",
    "**Note**: You can submit all 5 jobs at once or follow one at a time. \n",
    "\n",
    "After submission, they will go into a queue and run as soon as the requested compute resources become available. \n",
    "(tip: **shift+enter** will run the cell and automatically move you to the next cell. So you can hit **shift+enter** multiple times to quickly run multiple cells)\n",
    "\n",
    "**Note** :If you want to use your own video, Change the environment variable 'VIDEO' in the following cell from \"/data/reference-sample-data/safety-gear-detection/Safety_Full_Hat_and_Vest.mp4\" to the full path of your uploaded video.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"VIDEO\"] = \"/data/reference-sample-data/safety-gear-detection/Safety_Full_Hat_and_Vest.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting a job to a node with an 6th Gen Intel® CPU\n",
    "In the cell below, we submit a job to an edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel® \n",
    "    Core i5-6500TE</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_core = !qsub object_detection_job.sh -l nodes=1:idc001skl:tank-870:i5-6500te -F \"-r results/ -d CPU -f FP32 -i $VIDEO -n 2\" -N obj_det_core\n",
    "print(job_id_core[0]) \n",
    "#Progress indicators\n",
    "if job_id_core:\n",
    "    progressIndicator('results/', 'pre_progress_{}.txt'.format(job_id_core[0]), \"Preprocessing\", 0, 100)\n",
    "    progressIndicator('results/', 'i_progress_{}.txt'.format(job_id_core[0]), \"Inference\", 0, 100)\n",
    "    progressIndicator('results/', 'post_progress_{}.txt'.format(job_id_core[0]), \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting a job to a node with 6th Gen Intel® Xeon CPU\n",
    "In the cell below, we submit a job to an edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88178/Intel-Xeon-Processor-E3-1268L-v5-8M-Cache-2-40-GHz-\">Intel \n",
    "    Xeon Processor E3-1268L v5</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_xeon = !qsub object_detection_job.sh -l nodes=1:idc007xv5:e3-1268l-v5 -F \"-r results/ -d CPU -f FP32 -i $VIDEO -n 2\" -N obj_det_xeon \n",
    "print(job_id_xeon[0]) \n",
    "#Progress indicators\n",
    "if job_id_xeon:\n",
    "    progressIndicator('results/', 'pre_progress_{}.txt'.format(job_id_xeon[0]), \"Preprocessing\", 0, 100)\n",
    "    progressIndicator('results/', 'i_progress_{}.txt'.format(job_id_xeon[0]), \"Inference\", 0, 100)\n",
    "    progressIndicator('results/', 'post_progress_{}.txt'.format(job_id_xeon[0]), \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting a job to a node with Intel® HD Graphics 530  \n",
    "In the cell below, we submit a job to an edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core i5-6500TE</a>. The inference workload will run on the Intel® HD Graphics 530 card integrated with the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_gpu = !qsub object_detection_job.sh -l nodes=1:idc001skl:intel-hd-530 -F \"-r results/ -d GPU -f FP32 -i $VIDEO -n 4\" -N obj_det_gpu \n",
    "print(job_id_gpu[0]) \n",
    "#Progress indicators\n",
    "if job_id_gpu:\n",
    "    progressIndicator('results/', 'pre_progress_{}.txt'.format(job_id_gpu[0]), \"Preprocessing\", 0, 100)\n",
    "    progressIndicator('results/', 'i_progress_{}.txt'.format(job_id_gpu[0]), \"Inference\", 0, 100)\n",
    "    progressIndicator('results/', 'post_progress_{}.txt'.format(job_id_gpu[0]), \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting a job to a node with Intel® Arria® 10 FPGA\n",
    "In the cell below, we submit a job to an edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core i5-6500te CPU</a> . The inference workload will run on the <a href=\"https://www.ieiworld.com/mustang-f100/en/\">  Intel® Arria® 10 FPGA </a> card installed in this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_fpga = !qsub object_detection_job.sh -l nodes=1:idc003a10:iei-mustang-f100-a10 -F \"-r results/ -d HETERO:FPGA,CPU -f FP32 -i $VIDEO -n 4\" -N obj_det_fpga\n",
    "print(job_id_fpga[0]) \n",
    "#Progress indicators\n",
    "if job_id_fpga:\n",
    "    progressIndicator('results/', 'pre_progress_{}.txt'.format(job_id_fpga[0]), \"Preprocessing\", 0, 100)\n",
    "    progressIndicator('results/', 'i_progress_{}.txt'.format(job_id_fpga[0]), \"Inference\", 0, 100)\n",
    "    progressIndicator('results/', 'post_progress_{}.txt'.format(job_id_fpga[0]), \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting a job to a node with Intel® Neural Compute Stick 2\n",
    "In the cell below, we submit a job to an edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">6th Gen Intel Core i5-6500te CPU</a>. The inference workload will run on an <a \n",
    "    href=\"https://software.intel.com/en-us/neural-compute-stick\">Intel® Neural Compute Stick 2</a> installed in this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_ncs2 = !qsub object_detection_job.sh -l nodes=1:idc004nc2:intel-ncs2 -F \"-r results/ -d MYRIAD -f FP16 -i $VIDEO -n 8\" -N obj_det_ncs2\n",
    "print(job_id_ncs2[0]) \n",
    "#Progress indicators\n",
    "if job_id_ncs2:\n",
    "    progressIndicator('results/', 'pre_progress_{}.txt'.format(job_id_ncs2[0]), \"Preprocessing\", 0, 100)\n",
    "    progressIndicator('results/', 'i_progress_{}.txt'.format(job_id_ncs2[0]), \"Inference\", 0, 100)\n",
    "    progressIndicator('results/', 'post_progress_{}.txt'.format(job_id_ncs2[0]), \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting a job to a node with Intel® Movidius™ Myriad™ X Vision Processing Unit (VPU)\n",
    "In the cell below, we submit a job to an edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core i5-6500te CPU</a>. The inference workload will run on an <a \n",
    "    href=\"https://www.ieiworld.com/mustang-v100/en/\">IEI Mustang-V100-MX8 </a>accelerator installed in this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_hddlr = !qsub object_detection_job.sh -l nodes=1:idc002mx8:iei-mustang-v100-mx8 -F \"-r results/ -d HDDL -f FP16 -i $VIDEO -n 128\" -N obj_det_hddlr\n",
    "print(job_id_hddlr[0]) \n",
    "#Progress indicators\n",
    "if job_id_hddlr:\n",
    "    progressIndicator('results/', 'pre_progress_{}.txt'.format(job_id_hddlr[0]), \"Preprocessing\", 0, 100)\n",
    "    progressIndicator('results/', 'i_progress_{}.txt'.format(job_id_hddlr[0]), \"Inference\", 0, 100)\n",
    "    progressIndicator('results/', 'post_progress_{}.txt'.format(job_id_hddlr[0]), \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting a job to a node with Intel Atom and Intel® HD Graphics 505\n",
    "In the cell below, we submit a job to an edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/96488/Intel-Atom-x7-E3950-Processor-2M-Cache-up-to-2-00-GHz-\">Intel Atom® x7-E3950 Processor</a>. The inference  workload will run on the integrated Intel® HD Graphics 505 card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_up2 = !qsub object_detection_job.sh -l nodes=1:up-squared -F \"-r results/ -d GPU -f FP32 -i $VIDEO -n 2\" -N obj_det_up2\n",
    "print(job_id_up2[0]) \n",
    "#Progress indicators\n",
    "if job_id_up2:\n",
    "    progressIndicator('results/', 'pre_progress_{}.txt'.format(job_id_up2[0]), \"Preprocessing\", 0, 100)\n",
    "    progressIndicator('results/', 'i_progress_{}.txt'.format(job_id_up2[0]), \"Inference\", 0, 100)\n",
    "    progressIndicator('results/', 'post_progress_{}.txt'.format(job_id_up2[0]), \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Check if the jobs are done\n",
    "\n",
    "To check on the jobs that were submitted, use the `qstat` command.\n",
    "\n",
    "We have created a custom Jupyter widget  to get live qstat update.\n",
    "Run the following cell to bring it up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveQstat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the jobs you have submitted (referenced by `Job ID` that gets displayed right after you submit the job in step 2.3).\n",
    "There should also be an extra job in the queue \"jupyterhub\": this job runs your current Jupyter Notebook session.\n",
    "\n",
    "The 'S' column shows the current status. \n",
    "- If it is in Q state, it is in the queue waiting for available resources. \n",
    "- If it is in R state, it is running. \n",
    "- If the job is no longer listed, it means it is completed.\n",
    "\n",
    "**Note**: Time spent in the queue depends on the number of users accessing the edge nodes. Once these jobs begin to run, they should take from 1 to 5 minutes to complete. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Wait!***\n",
    "\n",
    "Please wait for the inference jobs and video rendering complete before proceeding to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## While you wait...\n",
    "\n",
    "### Why do we have three progress indicators for every job? \n",
    "\n",
    "In a real-life inference application, the video processing pipeline may have these phases:\n",
    "\n",
    "1. Preprocessing (such as video decoding, frame capture from camera, or fetching a frame from a network, resizing, cropping, contrast normalization, etc.) The performance of this phase will often depend on the CPU, storage, and networking speed.\n",
    "2. Inference (running a neural network forward propagation procedure). The performance of inference depends mostly on the compute device (CPU or an accelerator).\n",
    "3. Postprocessing (feeding data into the next pipeline stage, writing to file, sending it to a database, or even encoding a new video for a human viewer). This phase will often use the CPU, storage, or network, and will not use an accelerator.\n",
    "\n",
    "In our demonstration samples, we separate these three phases into separate workloads. This separation allows you to independently judge the relative impact of each phase and draw conclusions for your applications. \n",
    "\n",
    "### What does it mean for my application?\n",
    "\n",
    "Your application may have more or less pre/post-processing work depending on how you get your data (from storage, sensor or network), what is the resolution of your raw data, is it encoded or not, whether you need to produce an output video, etc. Therefore, you need to understand how your processing pipeline relates to this demo before drawing conclusions on performance.\n",
    "\n",
    "In this demo, we provide three performance numbers for each architecture. However, they are not hard limits on the solution's performance. You need to relate these numbers and code to what you are doing in your own project.\n",
    "\n",
    "### The total time is not necessarily equal to the sum of the parts\n",
    "\n",
    "It is important to realize that in a real-life application, you may want to fuse preprocessing, inference, and postprocessing, as opposed to separating them as we did here. Fused preprocessing has several advantages:\n",
    "* If inference is run asynchronously on the accelerator, the rest of the system is free for preprocessing future frames or postprocessing past frames. This allows you to mask the pre/post processing time behind inference time. \n",
    "* With fused preprocessing, inference and postprocessing, your pipeline has better data locality, allowing you to reuse data in the caches, in the application's memory, or in the hard drive cache. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: View Results\n",
    "\n",
    "Once the jobs are completed, the queue system outputs the stdout and stderr streams of each job into files with names of the form\n",
    "\n",
    "`obj_det_{type}.o{JobID}`\n",
    "\n",
    "`obj_det_{type}.e{JobID}`\n",
    "\n",
    "(here, obj_det_{type} corresponds to the `-N` option of qsub).\n",
    "\n",
    "However, for this case, we may be more interested in the output video files. They are stored in mp4 format inside the `results/` directory.\n",
    "We wrote a short utility script that will display these videos with in the notebook.\n",
    "Run the cells below to display them.\n",
    "See `demoutils.py` if you are interested in understanding further how the results are displayed in notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('6th Gen Intel® Core CPU', \n",
    "          ['results/output_{}.mp4'.format(job_id_core[0])], \n",
    "          'results/stats_{}.txt'.format(job_id_core[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('6th Gen Intel® Xeon CPU',\n",
    "          ['results/output_{}.mp4'.format(job_id_xeon[0])],\n",
    "          'results/stats_{}.txt'.format(job_id_xeon[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('6th Gen Intel® Core + Intel® HD Graphics 530', \n",
    "          ['results/output_{}.mp4'.format(job_id_gpu[0])],\n",
    "          'results/stats_{}.txt'.format(job_id_gpu[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('6th Gen Intel® Core + Intel® Arria® 10 FPGA',\n",
    "          ['results/output_{}.mp4'.format(job_id_fpga[0])],\n",
    "          'results/stats_{}.txt'.format(job_id_fpga[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('6th Gen Intel® Core + Intel® NCS2',\n",
    "          ['results/output_{}.mp4'.format(job_id_ncs2[0])],\n",
    "          'results/stats_{}.txt'.format(job_id_ncs2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('6th Gen Intel® Core + Intel® Movidius™ Myriad™ X Vision Processing Unit (VPU)',\n",
    "          ['results/output_{}.mp4'.format(job_id_hddlr[0])],\n",
    "          'results/stats_{}.txt'.format(job_id_hddlr[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('Intel® Atom + Intel® HD Graphics 505',\n",
    "          ['results/output_{}.mp4'.format(job_id_up2[0])],\n",
    "          'results/stats_{}.txt'.format(job_id_up2[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Assess Performance\n",
    "\n",
    "The running time of each inference task is recorded in `results/stats_job_id_{ARCH}.txt`, where the subdirectory name corresponds to the architecture of the target edge compute node. Run the cell below to plot the results of all jobs side-by-side. Lower values mean better performance. Keep in mind that some architectures are optimized for the highest performance, others for low power or other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_list = [('core', '6th Gen\\nIntel Core\\nCPU'),\n",
    "             ('xeon', '6th Gen\\nIntel Xeon\\nCPU'),\n",
    "             ('gpu', '6th Gen\\nIntel Core/\\nIntel HD\\nGraphics 530'),\n",
    "             ('fpga', '6th Gen\\nIntel Core/\\nIntel\\nArria10\\nFPGA'),\n",
    "             ('hddlr', '6th Gen\\nIntel Core/\\nMX8\\nVPU'),\n",
    "             ('ncs2', '6th Gen\\nIntel Core/\\nIntel\\nNCS2'),\n",
    "             ('up2', 'Intel Atom/\\nIntel HD\\nGraphics 505')]\n",
    "\n",
    "stats_list = []\n",
    "for arch, a_name in arch_list:\n",
    "    if 'job_id_'+arch in vars():\n",
    "        stats_list.append(('results/stats_'+vars()['job_id_'+arch][0]+'.txt', a_name))\n",
    "    else:\n",
    "        stats_list.append(('placeholder'+arch, a_name))\n",
    "\n",
    "summaryPlot(stats_list, 'Architecture', 'Time, seconds', 'Inference Engine Processing Time', 'time' )\n",
    "summaryPlot(stats_list, 'Architecture', 'Frames per second', 'Inference Engine FPS', 'fps' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
