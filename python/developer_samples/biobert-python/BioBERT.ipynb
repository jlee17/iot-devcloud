{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BioBERT Pre-trained Language Model for Biomedical Question Answering\n",
    "\n",
    "BioBERT is a biomedical language representation model designed for biomedical text mining tasks \\[[1](https://github.com/dmis-lab/bioasq-biobert)\\]. \n",
    "\n",
    "In this notebook, we demonstrate how BioBERT can be applied to perform biomedical question answering using the BioASQ dataset, and how we can leverage OpenVINO's [Deep Learning Inference Engine](https://docs.openvinotoolkit.org/latest/_docs_IE_DG_inference_engine_intro.html) to allow high performance inference on many different IntelÂ®  hardware types including CPU, GPU, FPGA, and VPU.\n",
    "\n",
    "The code in this example is adapted from the BioASQ BioBERT repository, which is based on the BERT repository. Links to both can be found in the references below.\n",
    "\n",
    "### References\n",
    "\n",
    "\\[[1](https://github.com/dmis-lab/bioasq-biobert)\\] BioASQ BioBERT Github Repository\n",
    "\n",
    "\\[[2](https://arxiv.org/ftp/arxiv/papers/1901/1901.08746.pdf)\\] Lee, Jinhyuk, et al. \"BioBERT: a pre-trained biomedical language representation model for biomedical text mining.\" Bioinformatics 36.4 (2020): 1234-1240.\n",
    "\n",
    "\\[[3](https://github.com/google-research/bert)\\] BERT Github Repository\n",
    "\n",
    "\\[[4](https://arxiv.org/pdf/1706.03762.pdf)\\] Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in neural information processing systems. 2017.\n",
    "\n",
    "\\[[5](https://arxiv.org/pdf/1810.04805.pdf)\\] Devlin, Jacob, et al. \"Bert: Pre-training of deep bidirectional transformers for language understanding.\" arXiv preprint arXiv:1810.04805 (2018).\n",
    "\n",
    "\\[[6](https://arxiv.org/pdf/1909.08229.pdf)\\] Yoon, Wonjin, et al. \"Pre-trained Language Model for Biomedical Question Answering.\" arXiv preprint arXiv:1909.08229 (2019).\n",
    "\n",
    "\\[[7](https://arxiv.org/pdf/1609.08144.pdf)\\] Wu, Yonghui, et al. \"Google's neural machine translation system: Bridging the gap between human and machine translation.\" arXiv preprint arXiv:1609.08144 (2016).\n",
    "\n",
    "\\[[8](http://www.bioasq.org/)\\] BioASQ Website\n",
    "\n",
    "\\[[9](https://github.com/BioASQ/Evaluation-Measures)\\] BioASQ Evaluatio Github Repository\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies\n",
    "\n",
    "Run the following cell to import the python dependencies for running the Tensorflow and OpenVINO examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import tokenization\n",
    "import modeling\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from openvino.inference_engine import IECore\n",
    "from multiprocessing import Process\n",
    "from run_factoid import write_predictions, read_squad_examples, convert_examples_to_features, model_fn_builder\n",
    "from tensorflow.contrib import predictor\n",
    "\n",
    "from qarpo.demoutils import *\n",
    "from time import time\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "def run_background(function):\n",
    "    p = Process(target=function)\n",
    "    p.start()\n",
    "    p.join()\n",
    "    return function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the BERT model\n",
    "\n",
    "<br>\n",
    "<figure>\n",
    "<img src=\"BioBERT_training.png\"/>\n",
    "<figcaption style=\"text-align:center\">BioBERT Training Methodology <a href=\"https://arxiv.org/ftp/arxiv/papers/1901/1901.08746.pdf\">[2]</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "BioBERT is initialized using the pre-trained weights from the original BERT model \\[[3](https://github.com/google-research/bert)\\] which is trained on BookCorpus and English Wikipedia data. Using that as a base, it additionally is pre-trained using PubMed abstracts for 1M steps (this is BioBERT 1.1).\n",
    "\n",
    "After that step it is fine tuned for named entity recogniton, relation extraction, and question answering. BioBERT demonstrates that doing additional training on domain specific information can improve performance for NLP tasks.\n",
    "\n",
    "The BERT model uses attention mechanisms \\[[4](https://arxiv.org/pdf/1706.03762.pdf)\\] to represent the relationships between a token an all other tokens . BERT is powerful not only because because it achieves state-of-the-art results, but also because generalizable to a wide array of different tasks such as machine translation, search, and question answering. \\[[5](https://arxiv.org/pdf/1810.04805.pdf)\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Tensorflow model from checkpoint file\n",
    "\n",
    "Before getting started, we need to first load the original Tensorflow checkpoint and export a saved model that we will use for inference and for converting to an OpenVINO intermediate representation (IR) later.\n",
    "\n",
    "The conversion process will display multiple warnings due to issues with a specific package version which can be solved by installing gast==0.2.2, but the errors should not affect the export process. More info about the warnings can be found [here](https://github.com/tensorflow/tensorflow/issues/32949)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@run_background\n",
    "def export():\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    bert_config = modeling.BertConfig.from_json_file('/data/BioBert/BERT-pubmed-1000000-SQuAD/bert_config.json')\n",
    "\n",
    "    run_config = tf.contrib.tpu.RunConfig(\n",
    "        model_dir='/data/BioBert/BERT-pubmed-1000000-SQuAD/',\n",
    "        tpu_config=None)\n",
    "\n",
    "    model_fn = model_fn_builder(\n",
    "        bert_config=bert_config,\n",
    "        init_checkpoint='/data/BioBert/BERT-pubmed-1000000-SQuAD/model.ckpt-14599',\n",
    "        learning_rate=0,\n",
    "        num_train_steps=0,\n",
    "        num_warmup_steps=0,\n",
    "        use_tpu=False,\n",
    "        use_one_hot_embeddings=False)\n",
    "\n",
    "    # If TPU is not available, this will fall back to normal Estimator on CPU\n",
    "    # or GPU.\n",
    "    estimator = tf.contrib.tpu.TPUEstimator(\n",
    "        use_tpu=False,\n",
    "        model_fn=model_fn,\n",
    "        config=run_config,\n",
    "        predict_batch_size=8)\n",
    "\n",
    "    features = {\n",
    "        \"input_ids\": tf.placeholder(shape=[None, 384], dtype=tf.int32, name='input_ids'),\n",
    "        \"input_mask\": tf.placeholder(shape=[None, 384], dtype=tf.int32, name='input_mask'),\n",
    "        \"segment_ids\": tf.placeholder(shape=[None, 384], dtype=tf.int32, name='segment_ids'),\n",
    "        \"unique_ids\": tf.placeholder(shape=[None], dtype=tf.int32, name='unique_ids'),\n",
    "        }\n",
    "    serving_input_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(features)\n",
    "    estimator._export_to_tpu = False  \n",
    "    estimator.export_saved_model(\n",
    "        export_dir_base='./tf_saved_model',\n",
    "        serving_input_receiver_fn=serving_input_fn)\n",
    "        #as_text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BioBERT Walkthrough\n",
    "\n",
    "The following section shows how input data is passed to the model and interpreted. We will go through the process of preparing inputs for the model and how to interpret the outputs. A diagram of how the BioBERT model is shown below for reference.\n",
    "\n",
    "<br>\n",
    "<figure>\n",
    "<img src=\"BioBERT_model.png\"/>\n",
    "<figcaption style=\"text-align:center\">BioBERT Model for Question Answering <a href=\"https://arxiv.org/pdf/1909.08229.pdf\">[6]</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "BioBERT question and answering relies on a context paragraph which contains the answer to a query that is passed in. An example of which is given in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context text is taken from https://www.who.int/health-topics/coronavirus\n",
    "example_context = \"Coronaviruses (CoV) are a large family of viruses that cause illness ranging from the common cold to more severe diseases such as Middle East Respiratory Syndrome (MERS-CoV) and Severe Acute Respiratory Syndrome (SARS-CoV). A novel coronavirus (nCoV) is a new strain that has not been previously identified in humans. Coronaviruses are zoonotic, meaning they are transmitted between animals and people.  Detailed investigations found that SARS-CoV was transmitted from civet cats to humans and MERS-CoV from dromedary camels to humans. Several known coronaviruses are circulating in animals that have not yet infected humans. Common signs of infection include respiratory symptoms, fever, cough, shortness of breath and breathing difficulties. In more severe cases, infection can cause pneumonia, severe acute respiratory syndrome, kidney failure and even death. Standard recommendations to prevent infection spread include regular hand washing, covering mouth and nose when coughing and sneezing, thoroughly cooking meat and eggs. Avoid close contact with anyone showing symptoms of respiratory illness such as coughing and sneezing.\"\n",
    "\n",
    "questions =  [\"What is MERS-CoV?\",\n",
    "              \"What are coronaviruses?\", \n",
    "              \"What are the common signs of coronavirus?\",\n",
    "              \"How are coronaviruses spread?\",\n",
    "              \"How do you prevent coronavirus infection?\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in the process is tokenizing the question and answer and converting them into a single input. In the cell below we load the vocabulary file, run the conversion using a tokenizer based on that file. \n",
    "\n",
    "Only a single question can be attached to a context at a time, so if there are multiple questions for a particular context paragraph, the process will need to be repeated for each question.\n",
    "\n",
    "The tokenizer uses WordPiece tokenization, meaning it checks each word and breaks any words not found in the vocabulary into multiple tokens which are a part of the vocabulary. The tokenizer uses \"##\" to mark any words that have been split apart using this method. This allows words that are not originally part of the vocabulary to be constructed from tokens, and is a more efficient than a purely character based approach. More information about WordPiece encodings can be found in in the in the paper by Wu et al. \\[[7](https://arxiv.org/pdf/1609.08144.pdf)\\]\n",
    "\n",
    "In our example, we can see that a word like \"large\" is included as a single token, while \"zoonotic\" is broken up into \"zoo\", \"##not\", and \"##ic\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = os.path.join(\"/data/BioBert/BERT-pubmed-1000000-SQuAD\", \"vocab.txt\")\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=False)\n",
    "\n",
    "question = questions[0]\n",
    "def tokenize(question, context):\n",
    "    return tokenizer.tokenize(question), tokenizer.tokenize(context)\n",
    "\n",
    "query_tokens, context_tokens = tokenize(question, example_context)\n",
    "\n",
    "print(\"Query tokens:\\n\", query_tokens)\n",
    "print(\"\\nContext tokens:\\n\", context_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the question and context converted into tokens, we need to pass it to the model for inference. The model requires three distinct inputs:\n",
    "\n",
    "- `input_ids` - a list of token numbers (corresponding to a line number in the vocab.txt file) for the input, which consists the question and the context concatenated together.\n",
    "- `input_mask` - used to indicate where the input has actual token values since the inputs are padded with 0's to ensure a specific length\n",
    "- `segment_ids` - distinguishes the question from the context, question and blank sections are 0's, context is 1's\n",
    "\n",
    "Note: The original Tensorflow model technically has a fourth input, `unique_id`, but it is not used in the inference process and is passed through the model without any modification, so it can be safely left out/replaced with a placeholder value.\n",
    "\n",
    "For the BioBERT model, each of the inputs needs to have an input length of 384, with shorter inputs being padded and longer inputs being broken into several context sections. As an additonal requirement, the beginning and the end of the question section of the input ids needs to have the \\[CLS\\] and \\[SEP\\] tokens, so an example input might look like \"\\[CLS\\] ... question ... \\[SEP\\] ... context ... \\[PAD\\]\". \n",
    "\n",
    "Runnning the cell below converts the tokens into the three inputs expected by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_inputs(query_tokens, context_tokens):\n",
    "    question_ids = tokenizer.convert_tokens_to_ids(query_tokens)\n",
    "    \n",
    "    # Add [CLS] and [SEP] tokens to question\n",
    "    question_ids.insert(0,101)\n",
    "    question_ids.append(102)\n",
    "    \n",
    "    context_ids = tokenizer.convert_tokens_to_ids(context_tokens)\n",
    "\n",
    "    question_len = len(question_ids)\n",
    "    context_len = len(context_ids)\n",
    "    \n",
    "    segment_ids = np.zeros((1,384), dtype=np.int32)\n",
    "    segment_ids[0,question_len:question_len+context_len] = 1\n",
    "    input_mask = np.zeros((1,384), dtype=np.int32)\n",
    "    input_mask[0,:question_len+context_len] = 1\n",
    "\n",
    "    # Concatenate the question and context, and 0 pad the result.\n",
    "    input_ids = np.expand_dims(np.concatenate((question_ids, context_ids, np.zeros((384-(question_len+context_len)), dtype=np.int32))),0)\n",
    "    \n",
    "    return input_ids, segment_ids, input_mask\n",
    "\n",
    "input_ids, segment_ids, input_mask = convert_inputs(query_tokens, context_tokens)\n",
    "\n",
    "print(\"Input ids:\\n\", input_ids)\n",
    "print(\"\\nInput mask:\\n\", input_mask)\n",
    "print(\"\\nSegment ids:\\n\", segment_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have prepared all of the necessary inputs, we can load the model and run the inference. For question answering, the outputs we expect from the model are predictions for where in the context paragraph the answer starts and where it ends. Since the model output indicates the starting and ending token of the answer, the final step in the process is converting the tokens back into words.\n",
    "\n",
    "Note: For maximum accuracy we would also want to: \n",
    "- ensure that the end point is not after the start point\n",
    "- limit the length of the answer\n",
    "- ensure that the answer does not contain the question itself\n",
    "- list alternative answers and their likelihood\n",
    "- clean up whitespace and unknown tokens in the output\n",
    "\n",
    "However, for the purposes of this demonstration, we just use the maximum value of the start and end outputs to determine where the answer is. The code that we will use later with the BioASQ dataset will do all of the above checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@run_background\n",
    "def tensorflow_inference():\n",
    "    start_time = time()\n",
    "    predict_fn = predictor.from_saved_model('./tf_saved_model/' + os.listdir('./tf_saved_model/')[0])\n",
    "    print(\"Model loaded in {} seconds\".format(time()-start_time))\n",
    "    start_time = time()\n",
    "    result = predict_fn({\"input_ids\": input_ids, \"segment_ids\": segment_ids, \"input_mask\": input_mask, \"unique_ids\": [1]})\n",
    "    print(\"Inference took {} seconds\".format(time()-start_time))\n",
    "    sl = result[\"start_logits\"][0,:]\n",
    "    el = result[\"end_logits\"][0,:]\n",
    "\n",
    "    answer = tokenizer.convert_ids_to_tokens(input_ids[0][np.argmax(sl):np.argmax(el)+1])\n",
    "    \n",
    "    print(\"\\n\" + question)\n",
    "    print(\"Answer tokens: \", answer)\n",
    "    print(\" \".join(answer).replace(\" ##\", \"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the Tensorflow .pb file to IR\n",
    "\n",
    "The next step in the process is running the OpenVINO Model Optimizer to generate an OpenVINO Intermediate Representation (IR) that uses FP16 precision. Due to the size of the model, we need to send the model conversion process to one of the edge nodes to run. \n",
    "\n",
    "The conversion process should take about 1-2 minutes. After the conversion is done, we can run the same inference as above using the OpenVINO model to ensure that the conversion was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qsub convert_tf_to_ov.sh -e logs/ -o logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the utility below to view the progress of the conversion. The outputs of the conversion process will be saved into the `logs/` folder. \n",
    "##### Wait for the conversion to complete before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveQstat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the inference using OpenVINO\n",
    "\n",
    "We can now run the same inference we just ran using the newly exported OpenVINO IR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@run_background\n",
    "def openvino_inference():\n",
    "    start_time = time()\n",
    "    ie = IECore()\n",
    "    net = ie.read_network(model = './ov/saved_model.xml', weights = './ov/saved_model.bin')\n",
    "    exec_net = ie.load_network(network=net, device_name='CPU')\n",
    "    del net\n",
    "    print(\"Model loaded in {} seconds\".format(time()-start_time))\n",
    "    \n",
    "    start_time = time()\n",
    "    result = exec_net.infer(inputs={\"input_ids\": input_ids, \"segment_ids\": segment_ids, \"input_mask\": input_mask})\n",
    "    print(\"Inference took {} seconds\".format(time()-start_time))\n",
    "    sl = result[\"unstack/Squeeze_\"][0,:]\n",
    "    el = result[\"unstack/Split.1\"][0,0,:]\n",
    "\n",
    "    answer = tokenizer.convert_ids_to_tokens(input_ids[0][np.argmax(sl):np.argmax(el)+1])\n",
    "    \n",
    "    print(\"\\n\" + question)\n",
    "    print(\"Answer tokens: \", answer)\n",
    "    print(\" \".join(answer).replace(\" ##\", \"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the OpenVINO inference on the BioASQ dataset\n",
    "\n",
    "Now that we have converted the model into an OpenVINO IR, we can run it on the BioASQ task V dataset \\[[8](http://www.bioasq.org/)\\] and evaluate its results. For the first step in the process, we load our saved model IR and set up an executable network that will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the OpenVINO IR\n",
    "ie = IECore()\n",
    "net = ie.read_network(model = './ov/saved_model.xml', weights = './ov/saved_model.bin')\n",
    "exec_net = ie.load_network(network=net, device_name='CPU')\n",
    "del net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the BioASQ dataset\n",
    "\n",
    "The BioASQ dataset contains excerpts from medical documents with questions and reference answers provided by a team of biomedical experts. The authors of the BioBERT model have converted the BioASQ data into the same format as the Stanford Question Answering Datset (SQuAD). We can see an example of the data by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -16 /data/BioBert/data-release/BioASQ-6b/test/Full-Abstract/BioASQ-test-factoid-6b-3.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the following cell will load one of the BioASQ test datasets. The `convert_examples_to_features` function processes all of the data using the same method as the example shown before. If you wish to run the inference again with a different dataset, you can modify the `input_file` line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this line if you want to test out the model with a different dataset\n",
    "input_file = \"/data/BioBert/data-release/BioASQ-6b/test/Full-Abstract/BioASQ-test-factoid-6b-3.json\"\n",
    "\n",
    "eval_examples = read_squad_examples(input_file=input_file,is_training=False)\n",
    "\n",
    "# These are the parameters specified from the BioBERT model\n",
    "max_seq_length = 384\n",
    "doc_stride = 128\n",
    "max_query_length = 64\n",
    "batch_size = 2\n",
    "n_best_size = 20\n",
    "max_answer_length = 30\n",
    "\n",
    "data_features = []\n",
    "\n",
    "def append_features(feature):\n",
    "    data_features.append(feature)\n",
    "\n",
    "# Use convert_examples_to_features method from run_factoid to convert the data\n",
    "convert_examples_to_features(eval_examples, tokenizer,\n",
    "                             max_seq_length, doc_stride,\n",
    "                             max_query_length, False,\n",
    "                             append_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the inference\n",
    "\n",
    "The outputs of the inference will be written to `predictions/predictions.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1\n",
    "n = len(data_features)\n",
    "all_results = []\n",
    "\n",
    "RawResult = collections.namedtuple(\"RawResult\", [\"unique_id\", \"start_logits\", \"end_logits\"])\n",
    "\n",
    "for idx in tqdm(range(n)):\n",
    "    data = {\"input_ids\": list(map(lambda x: x.input_ids, data_features[idx:idx+bs])),\n",
    "            \"input_mask\": list(map(lambda x: x.input_mask, data_features[idx:idx+bs])),\n",
    "            \"segment_ids\": list(map(lambda x: x.segment_ids, data_features[idx:idx+bs]))}\n",
    "\n",
    "    result = exec_net.infer(inputs=data)\n",
    "\n",
    "    in_batch = result[\"unstack/Squeeze_\"].shape[0]\n",
    "\n",
    "    for i in range(in_batch):\n",
    "        unique_id = 1000000000 + len(all_results)\n",
    "        sl = result[\"unstack/Squeeze_\"][i,:]\n",
    "        el = result[\"unstack/Split.1\"][0,i,:]\n",
    "        start_logits = [float(x) for x in sl.flat]\n",
    "        end_logits = [float(x) for x in el.flat]\n",
    "        all_results.append(RawResult(unique_id=unique_id, start_logits=start_logits, end_logits=end_logits))\n",
    "\n",
    "output_dir = \"predictions\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_prediction_file = os.path.join(output_dir, \"predictions.json\")\n",
    "output_nbest_file = os.path.join(output_dir, \"nbest_predictions.json\")\n",
    "output_null_log_odds_file = os.path.join(output_dir, \"null_odds.json\")\n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "write_predictions(eval_examples, data_features, all_results,\n",
    "                  n_best_size, max_answer_length,\n",
    "                  True, output_prediction_file, output_nbest_file,\n",
    "                  output_null_log_odds_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the output of the inference\n",
    "\n",
    "Run the following cell to view the results of the inference along with the original questions. \n",
    "\n",
    "It is also possible to run the official BioASQ evaluation measures to check the accuracy of the predictions \\[[9](https://github.com/BioASQ/Evaluation-Measures)\\]. However, we do not include this as part of the notebook since the official evaluation measures require Java, which is not included on the DevCloud. If you wish to run the evaluation locally, instructions for doing so can be found in the BioBERT repository \\[[1](https://github.com/dmis-lab/bioasq-biobert)\\] and the required golden answer datasets can be found at the BioASQ website ([6b](http://participants-area.bioasq.org/Tasks/6b/goldenDataset/), [7b](http://participants-area.bioasq.org/Tasks/6b/goldenDataset/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_filename = \"./predictions/predictions.json\"\n",
    "\n",
    "with open(input_file) as json_file2:\n",
    "    contexts = json.load(json_file2)\n",
    "\n",
    "with open(predictions_filename) as json_file:\n",
    "    answers = json.load(json_file)\n",
    "\n",
    "for data in answers.items():\n",
    "\n",
    "    id = data[0]\n",
    "    answer = data[1]\n",
    "    print(\"ID: {}\".format(id))\n",
    "    print(\"=\"*32)\n",
    "\n",
    "    for d in contexts[\"data\"][0][\"paragraphs\"]:\n",
    "        if d[\"qas\"][0][\"id\"] == id:\n",
    "            question = d[\"qas\"][0][\"question\"]\n",
    "            context = d[\"context\"]\n",
    "            print(\"Context:\\n********\\n{}\\n\\nQuestion:\\n*********\\n{}\\n\\nPrediction:\\n***********\\n{}\".format(context, question, answer))\n",
    "\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter your own context and questions\n",
    "\n",
    "You can try the model out for yourself below by filling in values for the context and questions below. Note that the answer to the question must be present in the context and that the context has a maximum length of 384 tokens for the BioBERT model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Enter in your own context and list of questions below and uncomment the lines\n",
    "# example_context = \"CONTEXT GOES HERE\"\n",
    "# questions = [\"QUESTIONS GO HERE\", ]\n",
    "\n",
    "\n",
    "for item in questions:\n",
    "    query_tokens, context_tokens = tokenize(item, example_context)\n",
    "    input_ids, segment_ids, input_mask = convert_inputs(query_tokens, context_tokens)\n",
    "    start_time = time()\n",
    "    result = exec_net.infer(inputs={\"input_ids\": input_ids, \"segment_ids\": segment_ids, \"input_mask\": input_mask})\n",
    "\n",
    "    sl = result[\"unstack/Squeeze_\"][0,:]\n",
    "    el = result[\"unstack/Split.1\"][0,0,:]\n",
    "\n",
    "    answer = tokenizer.convert_ids_to_tokens(input_ids[0][np.argmax(sl):np.argmax(el)+1])\n",
    "    \n",
    "    print(\"\\nInference took {} seconds\".format(time()-start_time))\n",
    "    print(item)\n",
    "    print(\" \".join(answer).replace(\" ##\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on the Edge\n",
    "\n",
    "All the code up to this point has been run within the Jupyter Notebook instance running on a development node based on an Intel Xeon Scalable processor. We will run the workload on other edge compute nodes represented in the IoT DevCloud. We will send work to the edge compute nodes by submitting the corresponding non-interactive jobs into a queue. For each job, we will specify the type of the edge compute server that must be allocated for the job.\n",
    "\n",
    "The job file is written in Bash, and will be executed directly on the edge compute node. For this example, we have written the job file for you in the notebook. It performs the classification using the script \"inference.sh\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile inference.sh\n",
    "\n",
    "cd $PBS_O_WORKDIR\n",
    "\n",
    "mkdir -p $1\n",
    "OUTPUT_DIR=$1\n",
    "DEVICE=$2\n",
    "\n",
    "if [ \"$DEVICE\" = \"HETERO:FPGA,CPU\" ]; then\n",
    "    # Environment variables and compilation for edge compute nodes with FPGAs - Updated for OpenVINO 2020.1\n",
    "    source /opt/altera/aocl-pro-rte/aclrte-linux64/init_opencl.sh\n",
    "    export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/opt/intel/openvino/bitstreams/a10_vision_design_sg1_bitstreams/BSP/a10_1150_sg1/linux64/lib\n",
    "    aocl program acl0 /opt/intel/openvino/bitstreams/a10_vision_design_sg1_bitstreams/2019R4_PL1_FP16_MobileNet_Clamp.aocx\n",
    "    export CL_CONTEXT_COMPILER_MODE_INTELFPGA=3\n",
    "fi\n",
    "\n",
    "python3 inference.py -d ${DEVICE} -o ${OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How jobs are submitted to the queue\n",
    "\n",
    "Now that we have the job script, we can submit the jobs to edge compute nodes. In the IoT DevCloud, you can do this using the `qsub` command.\n",
    "We can submit the job to 5 different types of edge compute nodes simultaneously or just one node at at time.\n",
    "\n",
    "There are five options of `qsub` command that we use for this:\n",
    "- `-l` : this option lets us select the number and the type of nodes using `nodes={node_count}:{property}`. \n",
    "- `-F` : this option lets us send arguments to the bash script. \n",
    "- `-N` : this option lets us name the job so that it is easier to distinguish between them.\n",
    "- `-o` : this option lets us determine the path to be used for the standard output stream.\n",
    "- `-e` : this option lets us determine the path to be used for the standard error stream.\n",
    "\n",
    "\n",
    "The `-F` flag is used to pass in arguments to the job script.\n",
    "The [inference.sh](inference.sh) script takes in 2 arguments:\n",
    "1. the path to the directory for the output video and performance stats\n",
    "2. targeted device (e.g. CPU, GPU, MYRIAD, HDDL or HETERO:FPGA,CPU)\n",
    "\n",
    "The job scheduler will use the contents of `-F` flag as the argument to the job script.\n",
    "\n",
    "If you are curious to see the available types of nodes on the IoT DevCloud, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pbsnodes | grep compnode | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the properties describe the node, and number on the left is the number of available nodes of that architecture.\n",
    "\n",
    "### Job queue submission\n",
    "\n",
    "The output of the cell is the `JobID` of your job, which you can use to track progress of a job.\n",
    "\n",
    "**Note** You can submit all the jobs at once or follow one at a time. \n",
    "\n",
    "After submission, they will go into a queue and run as soon as the requested compute resources become available. \n",
    "(tip: **shift+enter** will run the cell and automatically move you to the next cell. So you can hit **shift+enter** multiple times to quickly run multiple cells).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with an Intel Core CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel \n",
    "    Core i5-6500TE</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id_core = !qsub inference.sh -l nodes=1:idc001skl:i5-6500te -F \"results/core/ CPU\" -N BioBERT_core -e results/core/ -o results/core/   \n",
    "print(job_id_core[0]) \n",
    "#Progress indicators\n",
    "if job_id_core:\n",
    "    progressIndicator('./logs', job_id_core[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with an 8th Generation Intel Core CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/8th-gen-core-dev-kit\">UP Xtreme Edge Compute Enabling Kit\n",
    "    </a> edge node with a low power <a \n",
    "    href=\"https://ark.intel.com/content/www/us/en/ark/products/193554/intel-core-i7-8665ue-processor-8m-cache-up-to-4-40-ghz.html\">Intel \n",
    "    Core i7-8865UE</a>. The inference workload will run on the CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id_core2 = !qsub inference.sh -l nodes=1:idc014upxa10fx1 -F \"results/core2/ CPU\" -N BioBERT_core2 -e results/core2/ -o results/core2/\n",
    "print(job_id_core2[0]) \n",
    "#Progress indicators\n",
    "if job_id_core2:\n",
    "    progressIndicator('./logs', job_id_core2[0]+'.txt', \"Inference\", 0, 100)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel Xeon CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88178/Intel-Xeon-Processor-E3-1268L-v5-8M-Cache-2-40-GHz-\">Intel \n",
    "    Xeon Processor E3-1268L v5</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id_xeon = !qsub inference.sh -l nodes=1:idc007xv5:intel-xeon -F \"results/xeon/ CPU\" -N BioBERT_xeon -e results/xeon/ -o results/xeon/\n",
    "print(job_id_xeon[0]) \n",
    "#Progress indicators\n",
    "if job_id_xeon:\n",
    "    progressIndicator('./logs', job_id_xeon[0]+'.txt', \"Inference\", 0, 100)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with IntelÂ® Core CPU and using the onboard IntelÂ® GPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank* 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">IntelÂ® Core i5-6500TE</a>. The inference workload will run on the IntelÂ® HD Graphics 530 card integrated with the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id_gpu = !qsub inference.sh -l nodes=1:tank-870:i5-6500te:intel-hd-530 -F \"results/gpu/ GPU\" -N BioBERT_gpu -e results/gpu/ -o results/gpu/\n",
    "print(job_id_gpu[0]) \n",
    "#Progress indicators\n",
    "if job_id_gpu:\n",
    "    progressIndicator('./logs', job_id_gpu[0]+'.txt', \"Inference\", 0, 100)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with  IEI Mustang-F100-A10 (IntelÂ® ArriaÂ® 10 FPGA)\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core i5-6500te CPU</a> . The inference workload will run on the <a href=\"https://www.ieiworld.com/mustang-f100/en/\"> IEI Mustang-F100-A10 </a> card installed in this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id_fpga = !qsub inference.sh -l nodes=1:idc003a10:iei-mustang-f100-a10 -F \"results/fpga/ HETERO:FPGA,CPU\" -N BioBERT_fpga -e results/fpga/ -o results/fpga/\n",
    "print(job_id_fpga[0]) \n",
    "#Progress indicators\n",
    "if job_id_fpga:\n",
    "    progressIndicator('./logs', job_id_fpga[0]+'.txt', \"Inference\", 0, 100)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original Tensorflow model for Reference\n",
    "\n",
    "In the cell below we run the original Tensorflow model on an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel \n",
    "    Core i5-6500TE</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "job_id_tf = !qsub inference.sh -l nodes=1:idc001skl:i5-6500te -F \"results/tf/ TF\" -N BioBERT_tf -e results/tf/ -o results/tf/   \n",
    "print(job_id_tf[0]) \n",
    "#Progress indicators\n",
    "if job_id_tf:\n",
    "    progressIndicator('./logs', job_id_tf[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the job is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveQstat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the jobs you have submitted (referenced by `Job ID` that gets displayed right after you submit the job in step 2.3).\n",
    "There should also be an extra job in the queue \"jupyterhub\": this job runs your current Jupyter Notebook session.\n",
    "\n",
    "The 'S' column shows the current status. \n",
    "- If it is in Q state, it is in the queue waiting for available resources. \n",
    "- If it is in R state, it is running. \n",
    "- If the job is no longer listed, it means it is completed.\n",
    "\n",
    "**Note**: Time spent in the queue depends on the number of users accessing the edge nodes. Once these jobs begin to run, they should take from 1 to 5 minutes to complete. \n",
    "\n",
    "***Wait!***\n",
    "\n",
    "Please wait for the inference jobs complete before proceeding to the next step.\n",
    "\n",
    "### View Results\n",
    "\n",
    "We also saved the predicted answers as well as the performance data into the `results/` folder for each architecture.\n",
    "\n",
    "The results of each of the job submissions should be identical, and can be verified by comparing the results of the predictions.json file in each folder. The script below takes a small excerpt from each of the prediction files for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash compare.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess Performance\n",
    "\n",
    "The total average time of each inference task is recorded in `results/{ARCH}/stats_{job_id}.txt`, where the subdirectory name corresponds to the architecture of the target edge compute node. Run the cell below to plot the results of all jobs side-by-side. Lower values mean better performance. Keep in mind that some architectures are optimized for the highest performance, others for low power or other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_list = [('core', 'Intel Core\\ni5-6500TE\\nCPU'),\n",
    "             ('core2', 'Intel Core\\ni7-8865UE\\nCPU'),\n",
    "             ('xeon', 'Intel Xeon\\nE3-1268L v5\\nCPU'),\n",
    "             ('gpu', ' Intel Core\\ni5-6500TE\\nGPU'),\n",
    "             ('fpga', ' IEI Mustang\\nF100-A10\\nFPGA'),\n",
    "             ('tf', 'Original\\nTF Model on\\nIntel Core\\ni5-6500TE\\nCPU')]\n",
    "\n",
    "stats_list = []\n",
    "for arch, a_name in arch_list:\n",
    "    if 'job_id_'+arch in vars():\n",
    "        stats_list.append(('results/' + arch + '/stats_'+vars()['job_id_'+arch][0]+'.txt', a_name))\n",
    "    else:\n",
    "        stats_list.append(('placeholder'+arch, a_name))\n",
    "\n",
    "plt.ion()\n",
    "summaryPlot(stats_list, 'Architecture', 'Time, miliseconds', 'Processing Time Per Question', 'time' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
