{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pneumonia detection in chest X-ray images\n",
    "\n",
    "Pneumonia is an inflammatory condition of the lung affecting primarily the small air sacs known as alveoli.\n",
    "Typically symptoms include some combination of cough, chest pain, fever, and trouble breathing [1].\n",
    "Pneumonia affects approximately 450 million people globally (7% of the population) and results in about 4 million deaths per year [2]. To diagnose this disease, chest X-ray images remain the best diagnosis tool.\n",
    "\n",
    "![](example-pneumonia.jpeg) *Chest X-ray image of a patient with Pneumonia*\n",
    "\n",
    "In this tutorial, we use a model trained to classify patients with pneumonia over healthy cases based on their chest X-ray images. The topology used is the DenseNet 121, this architecture has shown to be very efficient at this problem, it was the first work to claim classification rate better than practicing radiologists. The dataset used for training is from the \"Labeled Optical Coherence Tomography (OCT) and Chest X-Ray Images for Classification\" [3] with a CC BY 4.0 license.\n",
    "The trained model is provided as a frozen Tensorflow model (.pb).\n",
    "\n",
    "\n",
    "**In this tutorial, we perform inference on this model using the OpenVINO(TM) toolkit on the validation dataset on multiple hardware targets such as different Intel CPUS, Neural Compute Stick, the Intel® Arria® 10 FPGA, etc...\n",
    "We will also visualize what the network has learned using the technique of Class Activation Maps.**\n",
    "\n",
    "[1] Ashby B, Turkington C (2007). The encyclopedia of infectious diseases (3rd ed.). New York: Facts on File. p. 242. ISBN 978-0-8160-6397-0. \n",
    "\n",
    "[2] Ruuskanen O, Lahti E, Jennings LC, Murdoch DR (April 2011). \"Viral pneumonia\". Lancet. 377 (9773): 1264–75. \n",
    "\n",
    "[3] Labeled Optical Coherence Tomography (OCT) and Chest X-Ray Images for Classification, http://dx.doi.org/10.17632/rscbjbr9sj.2#file-41d542e7-7f91-47f6-9ff2-dd8e5a5a7861"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to convert the model to OpenVINO Intermediate Representation through the Model Optimizer. The MO script is *mo_tf.py*, which takes as input:  \n",
    "-m: path to the input model (.pb)  \n",
    "\n",
    "--input_shape: dimension of the input tensor (optional for most of the models)\n",
    "\n",
    "--data_type: data type of the IR model (FP32 for CPU, FP16 for GPU, Movidius and FPGA)\n",
    "\n",
    "-o : path where to save the IR\n",
    "\n",
    "--mean_values : pre-processing mean values with which the model was trained with \n",
    "\n",
    "--scale_values : pre-processing scale values with which the model was trained with \n",
    "\n",
    "The second argument is not necessary for  models with a defined input shape. However, Tensorflow models often have dynamic input shapes (often the batch will be set as -1), which is not supported by OpenVINO.\n",
    "The mean and scale values are also optional, it however simplifies the inference later as it includes automatically the pre-processing (scaling and averaging) inside the model directly. For this model, the model was trained with images following the ImageNet pre-processing. \n",
    "\n",
    "First, we convert the model to a FP32 IR format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "!python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_tf.py -m model.pb --input_shape=[1,224,224,3] --data_type FP32 -o models/FP32 --mean_values [123.75,116.28,103.58] --scale_values [58.395,57.12,57.375] \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also convert to FP16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "!python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_tf.py -m model.pb --input_shape=[1,224,224,3] --data_type FP16 -o models/FP16/ --mean_values [123.75,116.28,103.58] --scale_values [58.395,57.12,57.375]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Both command lines above have been commented as the model has an issue on the current OpenVINO release used by the system. It is already fixed in next release.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the resulting Intermediate Representation (.xml). You can observe the different layers with informations such as name, operation type, precision, input shape and output shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from bs4 import BeautifulSoup\n",
    "\n",
    "# bs = BeautifulSoup(open('models/FP32/model.xml'), 'xml')\n",
    "# print(bs.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we are able to perform inference with OpenVINO. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification and Class Activation Maps using OpenVINO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will explain the sample that we will send in Section 3 to the edge devices. The sample does not only do pneumonia classification, but it also computes the Class Activation Maps. \n",
    "The code sample can be found [here](classification_pneumonia.py), we will break it down in the next subsections.\n",
    "Another tutorial  explains in details how to use the Python API for classification, therefore we refer the developer to [this tutorial](../../Tutorials/classification/tutorial_classification_sample.ipynb) for deep dive on this part. In this section, we will focus on the Class Activation Maps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize what the model has learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class activation maps (CAM) [1] are a simple technique to visualize the regions that are relevant to a Convolutional Neural Network to identify a specific class in the image. \n",
    "The CAM $M(x,y)$ is calculated from the N feature maps $f_i(x,y)$ from the last convolutional layer. We perform the weighted sum of those feature maps based on the weigths of the fully connected layer $w_i$ , which represents how important are those feaure maps for the classification output. \n",
    "\n",
    "$ M(x,y)=\\sum_{i=0}^N w_i f_i(x,y) $\n",
    "\n",
    "[1] Zhou, Bolei, et al. \"Learning deep features for discriminative localization.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n",
    "\n",
    "In OpenVINO, this can be implemented the following way:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def class_activation_map_openvino(res, convb, fc, net, fp16):\n",
    "    res_bn = res[convb]\n",
    "    conv_outputs=res_bn[0,:,:,:]\n",
    "    weights_fc=net.layers.get(fc).weights[\"weights\"]\n",
    "    cam = np.zeros(dtype=np.float32, shape=conv_outputs.shape[1:])\n",
    "    \n",
    "    for i, w in enumerate(weights_fc):\n",
    "        conv_outputs1=conv_outputs[i, :, :]\n",
    "        if fp16:\n",
    "            w=float16_conversion(w)\n",
    "            conv_outputs1=float16_conversion_array(conv_outputs[i, :, :])\n",
    "        cam += w * conv_outputs1\n",
    "    return cam\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate Class Activation Maps in OpenVINO, we need to access the output feature maps of the last convolution layer (here, named **$convb$**) and the weights of the fully connected layer (**$fc$**). \n",
    "By default, only the output layer is the output (obviously). Therefore, in order to get the feature maps, we need to add the last convolution **$convb$** as an additional output. \n",
    "This is simply done by using the function *add_outputs(convb)* on the network before loading it to the plugin. \n",
    "In order to obtain the layers name, we recommend Netron, which allows to visualize graphically the model. \n",
    "\n",
    "In our pneumonia classification model, the name of the last convolutional layer **$convb$** is  **relu_1/Relu**.\n",
    "The name of the Fully-Connected layer **$fc$** is **predictions_1/MatMul**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the feature maps, inference must be performed first. This is why our function to calculate the Class Action Maps requires the inference output **$res$** (which includes the feature maps since we added an additional output previously).\n",
    "We access the FC layer weights using the call  *net.layers.get(fc).weights[\"weights\"]* on the network **$net$**.\n",
    "\n",
    "Then, before doing the weighted sum, if we are using the FP16 model, we need to convert the weights and feature maps value to float. \n",
    "\n",
    "Finally, we perform the weighted sum of the weights with the feature maps. The result of the function is an image of same size as the feature maps (here, $7\\times7$).\n",
    "We show an example below on the left.\n",
    "Then, we can upsampled the CAM image to the original input image size and overlay it over the input image. The region of highest value (here, in yellow) will be the region that was the most relevant to decide on the classification value. \n",
    "\n",
    "![](example_CAM.png) *CAM image and overlay of CAM with input image*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inference on the edge\n",
    "\n",
    "\n",
    "We will run the workload on edge compute nodes represented in the IoT DevCloud. We will send work to the edge compute nodes by submitting the corresponding non-interactive jobs into a queue. For each job, we will specify the type of the edge compute server that must be allocated for the job.\n",
    "\n",
    "The job file is written in Bash, and will be executed directly on the edge compute node.\n",
    "For this example, we have written the job file for you in the notebook. You can look inside the Python script in [classification_pneumonia.py](classification_pneumonia.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path().resolve().parent.parent))\n",
    "from demoTools.demoutils import *\n",
    "from utils_image import show_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile classification_pneumonia_job.sh\n",
    "\n",
    "cd $PBS_O_WORKDIR\n",
    "\n",
    "OUTPUT_FILE=$1\n",
    "DEVICE=$2\n",
    "FP_MODEL=$3\n",
    "\n",
    "if [ \"$2\" = \"HETERO:FPGA,CPU\" ]; then\n",
    "    source /opt/intel/init_openvino.sh\n",
    "    aocl program acl0 /opt/intel/openvino/bitstreams/a10_vision_design_sg2_bitstreams/2019R3_PV_PL2_FP16_AlexNet_GoogleNet_InceptionV1_SSD300_Generic.aocx\n",
    "fi\n",
    "\n",
    "SAMPLEPATH=${PBS_O_WORKDIR}\n",
    "echo ${1}\n",
    "pip3 install Pillow\n",
    "python3 classification_pneumonia.py  -m models/$FP_MODEL/model.xml  \\\n",
    "                                           -i /validation_images/PNEUMONIA/*.jpeg \\\n",
    "                                           -o $OUTPUT_FILE \\\n",
    "                                           -d $DEVICE\n",
    "                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Understand how jobs are submitted into the queue\n",
    "\n",
    "Now that we have the job script, we can submit the jobs to edge compute nodes. In the IoT DevCloud, you can do this using the `qsub` command.\n",
    "We can submit classification_pneumonia_job to 6 different types of edge compute nodes simultaneously or just one node at at time.\n",
    "\n",
    "There are three options of `qsub` command that we use for this:\n",
    "- `-l` : this option lets us select the number and the type of nodes using `nodes={node_count}:{property}`. \n",
    "- `-F` : this option lets us send arguments to the bash script. \n",
    "- `-N` : this option lets use name the job so that it is easier to distinguish between them.\n",
    "\n",
    "The `-F` flag is used to pass in arguments to the job script.\n",
    "The [classification_pneumonia_job.sh](classification_pneumonia_job.sh) takes in 3 arguments:\n",
    "1. the path to the directory for the output video and performance stats\n",
    "2. targeted device (e.g. CPU,GPU,MYRIAD)\n",
    "3. the floating precision to use for inference\n",
    "\n",
    "The job scheduler will use the contents of `-F` flag as the argument to the job script.\n",
    "\n",
    "If you are curious to see the available types of nodes on the IoT DevCloud, run the following optional cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pbsnodes | grep compnode | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the properties describe the node, and number on the left is the number of available nodes of that architecture.\n",
    "\n",
    "### 3.2 Job queue submission\n",
    "\n",
    "The output of the cell is the `JobID` of your job, which you can use to track progress of a job.\n",
    "\n",
    "**Note** You can submit all 6 jobs at once or follow one at a time. \n",
    "\n",
    "After submission, they will go into a queue and run as soon as the requested compute resources become available. \n",
    "(tip: **shift+enter** will run the cell and automatically move you to the next cell. So you can hit **shift+enter** multiple times to quickly run multiple cells).\n",
    "\n",
    "The estimated time for each job does not only include the inference but also the rendering and writing on filesystem of the results, which is actually the most time-consuming part. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with an Intel Core CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel \n",
    "    Core i5-6500TE</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_core = !qsub classification_pneumonia_job.sh -l nodes=1:tank-870:i5-6500te -F \"results/core CPU FP32\" -N obj_det_core\n",
    "print(job_id_core[0]) \n",
    "if job_id_core:\n",
    "    progressIndicator('results/core', 'progress'+job_id_core[0]+'.txt', \"Progress\", 0, 100)\n",
    "#Progress indicators\n",
    "if not job_id_core:\n",
    "    print(\"Error in job submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel Xeon CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88178/Intel-Xeon-Processor-E3-1268L-v5-8M-Cache-2-40-GHz-\">Intel \n",
    "    Xeon Processor E3-1268L v5</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_xeon = !qsub classification_pneumonia_job.sh -l nodes=1:tank-870:e3-1268l-v5 -F \"results/xeon CPU FP32\" -N obj_det_xeon\n",
    "print(job_id_xeon[0]) \n",
    "if job_id_xeon:\n",
    "    progressIndicator('results/xeon', 'progress'+job_id_xeon[0]+'.txt', \"Progress\", 0, 100)\n",
    "if not job_id_xeon:\n",
    "    print(\"Error in job submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel Core CPU and using the onboard Intel GPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core i5-6500TE</a>. The inference workload will run on the Intel® HD Graphics 530 card integrated with the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_gpu = !qsub classification_pneumonia_job.sh -l nodes=1:tank-870:i5-6500te:intel-hd-530 -F \"results/gpu GPU FP16\" -N obj_det_gpu \n",
    "print(job_id_gpu[0]) \n",
    "if job_id_gpu:\n",
    "    progressIndicator('results/gpu', 'progress'+job_id_gpu[0]+'.txt', \"Progress\", 0, 100)\n",
    "if not job_id_gpu:\n",
    "    print(\"Error in job submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with  IEI Mustang-F100-A10 (Intel® Arria® 10 FPGA)\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core i5-6500te CPU</a> . The inference workload will run on the <a href=\"https://www.ieiworld.com/mustang-f100/en/\"> IEI Mustang-F100-A10 </a> card installed in this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_fpga = !qsub classification_pneumonia_job.sh -l nodes=1:tank-870:i5-6500te:iei-mustang-f100-a10 -F \"results/fpga HETERO:FPGA,CPU FP16\" -N obj_det_fpga\n",
    "print(job_id_fpga[0]) \n",
    "if job_id_fpga:\n",
    "    progressIndicator('results/fpga', 'progress'+job_id_fpga[0]+'.txt', \"Progress\", 0, 100)\n",
    "if not job_id_fpga:\n",
    "    print(\"Error in job submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel NCS 2 (Neural Compute Stick 2)\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core i5-6500te CPU</a>. The inference workload will run on an <a \n",
    "    href=\"https://software.intel.com/en-us/neural-compute-stick\">Intel Neural Compute Stick 2</a> installed in this  node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_myriadx = !qsub classification_pneumonia_job.sh -l nodes=1:tank-870:i5-6500te:intel-ncs2 -F \"results/myriadx MYRIAD FP16 \" -N obj_det_myriadx\n",
    "print(job_id_myriadx[0]) \n",
    "if job_id_myriadx:\n",
    "    progressIndicator('results/myriadx', 'progress'+job_id_myriadx[0]+'.txt', \"Progress\", 0, 100)\n",
    "if not job_id_myriadx:\n",
    "    print(\"Error in job submission.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with UP Squared Grove IoT Development Kit (UP2)\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/up-squared-grove-dev-kit\">UP Squared Grove IoT Development Kit</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/96488/Intel-Atom-x7-E3950-Processor-2M-Cache-up-to-2-00-GHz-\">Intel Atom® x7-E3950 Processor</a>. The inference  workload will run on the integrated Intel® HD Graphics 505 card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_up2 = !qsub classification_pneumonia_job.sh -l nodes=1:up-squared -F \"results/up2 GPU FP16\" -N obj_det_up2\n",
    "print(job_id_up2[0]) \n",
    "if job_id_up2:\n",
    "    progressIndicator('results/up2', 'progress'+job_id_up2[0]+'.txt', \"Progress\", 0, 100)\n",
    "if not job_id_up2:\n",
    "    print(\"Error in job submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Check if the jobs are done\n",
    "\n",
    "To check on the jobs that were submitted, use the `qstat` command.\n",
    "\n",
    "We have created a custom Jupyter widget  to get live qstat update.\n",
    "Run the following cell to bring it up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveQstat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the jobs you have submitted (referenced by `Job ID` that gets displayed right after you submit the job in step 2.3).\n",
    "There should also be an extra job in the queue \"jupyterhub\": this job runs your current Jupyter Notebook session.\n",
    "\n",
    "The 'S' column shows the current status. \n",
    "- If it is in Q state, it is in the queue waiting for available resources. \n",
    "- If it is in R state, it is running. \n",
    "- If the job is no longer listed, it means it is completed.\n",
    "\n",
    "**Note**: Time spent in the queue depends on the number of users accessing the edge nodes. Once these jobs begin to run, they should take from 1 to 5 minutes to complete. \n",
    "\n",
    "***Wait!***\n",
    "\n",
    "Please wait for the inference jobs complete before proceeding to the next step.\n",
    "\n",
    "### 3.4 View Results\n",
    "\n",
    "Once the jobs are completed, the queue system outputs the stdout and stderr streams of each job into files with names of the form\n",
    "\n",
    "`obj_det_{type}.o{JobID}`\n",
    "\n",
    "`obj_det_{type}.e{JobID}`\n",
    "\n",
    "(here, obj_det_{type} corresponds to the `-N` option of qsub).\n",
    "\n",
    "We also saved the probability output and inference time for each input image in the folder `results/` for each architecture. \n",
    "We observe the results below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result on the Intel Core CPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_results(job_id_core[0], 'core')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result on the Intel Xeon CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_results(job_id_xeon[0], 'xeon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result on the Intel Integrated GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_results(job_id_gpu[0], 'gpu')\n",
    "result_file=\"results/gpu/result\"+job_id_gpu[0]+\".txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result on the IEI Mustang-F100-A10 (Intel® Arria® 10 FPGA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_results(job_id_fpga[0], 'fpga')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result on the Intel NCS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_results(job_id_myriadx[0], 'myriadx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result on the UP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_results(job_id_up2[0], 'up2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Assess Performance\n",
    "\n",
    "The total average time of each inference task is recorded in `results/{ARCH}/statsjob_id.txt`, where the subdirectory name corresponds to the architecture of the target edge compute node. Run the cell below to plot the results of all jobs side-by-side. Lower values mean better performance. Keep in mind that some architectures are optimized for the highest performance, others for low power or other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_list = [('core', 'Intel Core\\ni5-6500TE\\nCPU'),\n",
    "             ('xeon', 'Intel Xeon\\nE3-1268L v5\\nCPU'),\n",
    "             ('gpu', ' Intel Core\\ni5-6500TE\\nGPU'),\n",
    "             ('fpga', ' IEI Mustang\\nF100-A10\\nFPGA'),\n",
    "             ('myriadx', 'Intel\\nNCS2'),\n",
    "             ('up2', 'Intel Atom\\nx7-E3950\\nUP2/GPU')]\n",
    "\n",
    "stats_list = []\n",
    "for arch, a_name in arch_list:\n",
    "    if 'job_id_'+arch in vars():\n",
    "        stats_list.append(('results/'+arch+'/stats'+vars()['job_id_'+arch][0]+'.txt', a_name))\n",
    "    else:\n",
    "        stats_list.append(('placeholder'+arch, a_name))\n",
    "\n",
    "#print(stats_list)\n",
    "\n",
    "summaryPlot(stats_list, 'Architecture', 'Time, milliseconds', 'Inference Engine Processing Time', 'time' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
