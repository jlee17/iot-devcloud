{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='red'>  Important: </font> Before proceeding, run the following cell to check for code updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qarpo.catalog import DemoCatalog\n",
    "import os\n",
    "status = DemoCatalog(os.getcwd(), \"Demo\").ShowRepositoryControls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR (Optical Character Recognition)\n",
    "\n",
    "This is an example for OCR (Optical Character Recognition) using the Intel® Distribution of OpenVINO™ toolkit.  \n",
    "We will use the Convolutional Recurrent Neural Networks (CRNN) for Scene Text Recognition from the following github page : https://github.com/MaybeShewill-CV/CRNN_Tensorflow\n",
    "\n",
    "To obtain the frozen model necessary to start with the Intel® Distribution of OpenVINO™ toolkit from the github repository, please look at our [documentation](https://docs.openvinotoolkit.org/R5/_docs_MO_DG_prepare_model_convert_model_tf_specific_Convert_CRNN_From_Tensorflow.html) \n",
    "\n",
    "In this tutorial, we will show you first how to convert the TF (TensorFlow) frozen model through the Model Optimizer, then we will perform inference on the CPU (first Intel® Xeon® CPU and then Intel® Core™ CPUs).\n",
    "As the CRNN includes a LSTM (Long Short-term Memory) cell, the inference can only be performed on CPU (the only hardware plugin to support this layer yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup the Python environement \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import warnings\n",
    "from qarpo.demoutils import *\n",
    "\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Optimizer\n",
    "\n",
    "\n",
    "Model Optimizer creates Intermediate Representation (IR) models that are optimized for inference. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/openvino/deployment_tools/model_optimizer/mo_tf.py \\\n",
    "--input_model model/crnn.pb \\\n",
    "--data_type FP32 \\\n",
    "-o model/FP32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** the above line is a single command line input, which spans 4 lines thanks to the backslash '\\\\', which is a line continuation character in Bash.\n",
    "\n",
    "Here, the arguments are:\n",
    "* --input-model : the original model\n",
    "* --data_type : Data type to use. One of {FP32, FP16, half, float}\n",
    "* -o : output directory\n",
    "\n",
    "This script also supports `-h` that will you can get the full list of arguments.\n",
    "\n",
    "With the `-o` option set as above, this command will write the output to the directory `model/FP32`\n",
    "\n",
    "There are two files produced:\n",
    "```\n",
    "models/FP32/crnn.xml\n",
    "models/FP32/crnn.bin\n",
    "```\n",
    "These will be used later in the exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inference Engine\n",
    "\n",
    "Now, we will run the inference on this model by building progressively the Python sample required to perform inference. \n",
    "This part of exercise features our Python API, similar functionalities can be found in our C++ API too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do OCR on the following input image, which obviously reads as **Industries**.\n",
    "![Image](board4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the CPU extension lib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/openvino/deployment_tools/inference_engine/samples/build_samples.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from easydict import EasyDict\n",
    "    print(\"EasyDict installed\")\n",
    "except:\n",
    "    !pip3 install --user easydict\n",
    "    print(\"To import easydict, please restart the kernel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "from argparse import ArgumentParser\n",
    "import cv2\n",
    "import numpy as np\n",
    "import logging as log\n",
    "from time import time\n",
    "from openvino.inference_engine import IENetwork, IECore\n",
    "from local_utils import log_utils, data_utils\n",
    "from local_utils.config_utils import load_config\n",
    "import os.path as ops\n",
    "from easydict import EasyDict\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define variables  like model path, target device, the codec for letter conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xml='model/FP32/crnn.xml'\n",
    "device_arg='CPU'\n",
    "input_arg=['board4.jpg']\n",
    "iterations=1\n",
    "perf_counts=False\n",
    "\n",
    "codec = data_utils.TextFeatureIO(char_dict_path='Config/char_dict.json',ord_map_dict_path=r'Config/ord_map.json')\n",
    "log.basicConfig(format=\"[ %(levelname)s ] %(message)s\", level=log.INFO, stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference Engine initialization  and load extensions library\n",
    "We initialize the Inference Engine by calling the class **`IECore()`** load the extensions library (if required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = IECore()\n",
    "ie.add_extension(os.path.join(os.getenv('HOME'), 'inference_engine_samples_build/intel64/Release/lib/libcpu_extension.so'), \"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read IR\n",
    "We can import optimized models (weights) from step 1 into our neural network using **`IENetwork`**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bin = os.path.splitext(model_xml)[0] + \".bin\"\n",
    "net = IENetwork(model=model_xml, weights=model_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing input blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_blob = next(iter(net.inputs))\n",
    "out_blob = next(iter(net.outputs))\n",
    "net.batch_size = len(input_arg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read and pre-process input images\n",
    "First let's load the image using OpenCV.\n",
    "We will also have to do some shape manipulation to convert the image to a format that is compatible with our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, c, h, w = net.inputs[input_blob].shape\n",
    "images = np.ndarray(shape=(n, c, h, w))\n",
    "for i in range(n):\n",
    "    image = cv2.imread(input_arg[i])\n",
    "    if image.shape[:-1] != (h, w):\n",
    "        log.warning(\"Image {} is resized from {} to {}\".format(input_arg[i], image.shape[:-1], (h, w)))\n",
    "        image = cv2.resize(image, (w, h))\n",
    "    image = image.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n",
    "    images[i] = image\n",
    "log.info(\"Batch size is {}\".format(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading model to the plugin\n",
    "Once we have the Inference Engine and the network, we can load the network into the Inference Engine using **`ie.load`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_net = ie.load_network(network=net, device_name=device_arg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Inference\n",
    "We can now run the inference on the object  **`exec_net`** using the function infer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_time = []\n",
    "for i in range(iterations):\n",
    "    t0 = time()\n",
    "    res = exec_net.infer(inputs={input_blob: images})\n",
    "    infer_time.append((time()-t0)*1000)\n",
    "\n",
    "res = res[out_blob]\n",
    "    \n",
    "log.info(\"Average running time of one iteration: {} ms\".format(np.average(np.asarray(infer_time))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing output blob\n",
    "The network outputs a tensor of dimension 25 (string length) * 37 (dimension of character space).\n",
    "First, we will go through the 25 characters and extracts the highest probability in the character space and its index in this space. \n",
    "We use the encoding files from the Github page to recover the mapping from index to character. (0&rarr;\"a\",36&rarr;\" \")\n",
    "In the github page, they also remove the consecutive duplicates and the space char, therefore we also perform this postprocessing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = res.argmax(2) ## extract highest probability in the second dimension\n",
    "preds = preds.transpose(1, 0)\n",
    "preds = np.ascontiguousarray(preds, dtype=np.int8).view(dtype=np.int8) # reformat to an array \n",
    "values=codec.writer.ordtochar( preds[0].tolist()) # map from index to character\n",
    "values=[v for i, v in enumerate(values) if i == 0 or v != values[i-1]] # remove duplicates\n",
    "values = [x for x in values if x != ' '] # remove space char (was character from index 36)\n",
    "res=''.join(values)\n",
    "print(\"The result is : \" + res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Job submission\n",
    "\n",
    "All the code up to this point has been run within the Jupyter Notebook instance running on a development node based on an Intel Xeon Scalable processor, where the Notebook is allocated a single core. \n",
    "We will run the workload on other edge compute nodes represented in the IoT DevCloud. We will send work to the edge compute nodes by submitting the corresponding non-interactive jobs into a queue. For each job, we will specify the type of the edge compute server that must be allocated for the job.\n",
    "\n",
    "The job file is written in Bash, and will be executed directly on the edge compute node.\n",
    "For this example, we have written the job file for you in the notebook.\n",
    "Run the following cell to write this in to the file \"ocr_job.sh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ocr_job.sh\n",
    "\n",
    "# The default path for the job is your home directory, so we change directory to where the files are.\n",
    "cd $PBS_O_WORKDIR\n",
    "mkdir -p $1\n",
    "SAMPLEPATH=$PBS_O_WORKDIR\n",
    "python3 classification_sample.py  -m model/$3/crnn.xml  \\\n",
    "                                           -i board4.jpg \\\n",
    "                                           -o $1 \\\n",
    "                                           -d $2 \\\n",
    "                                           -l ~/inference_engine_samples_build/intel64/Release/lib/libcpu_extension.so\n",
    "                                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Understand how jobs are submitted into the queue\n",
    "\n",
    "Now that we have the job script, we can submit the jobs to edge compute nodes. In the IoT DevCloud, you can do this using the `qsub` command.\n",
    "We can submit ocr_job to 5 different types of edge compute nodes simultaneously or just one node at at time.\n",
    "\n",
    "There are three options of `qsub` command that we use for this:\n",
    "- `-l` : this option lets us select the number and the type of nodes using `nodes={node_count}:{property}`. \n",
    "- `-F` : this option lets us send arguments to the bash script. \n",
    "- `-N` : this option lets use name the job so that it is easier to distinguish between them.\n",
    "\n",
    "The `-F` flag is used to pass in arguments to the job script.\n",
    "The [ocr.sh](ocr_job.sh) takes in 4 arguments:\n",
    "1. the path to the directory for the output video and performance stats\n",
    "2. targeted device (e.g. CPU,GPU,MYRIAD)\n",
    "3. the floating precision to use for inference\n",
    "4. the path to the input video\n",
    "\n",
    "The job scheduler will use the contents of `-F` flag as the argument to the job script.\n",
    "\n",
    "If you are curious to see the available types of nodes on the IoT DevCloud, run the following optional cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pbsnodes | grep compnode | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the properties describe the node, and number on the left is the number of available nodes of that architecture.\n",
    "\n",
    "### 3.2 Job queue submission\n",
    "\n",
    "The output of the cell is the `JobID` of your job, which you can use to track progress of a job.\n",
    "\n",
    "**Note** You can submit all 5 jobs at once or follow one at a time. \n",
    "\n",
    "After submission, they will go into a queue and run as soon as the requested compute resources become available. \n",
    "(tip: **shift+enter** will run the cell and automatically move you to the next cell. So you can hit **shift+enter** multiple times to quickly run multiple cells).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run on Intel® Xeon® E3 CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting a job to an edge compute node with an Intel Xeon CPU...\")\n",
    "#Submit job to the queue\n",
    "job_id_xeon = !qsub ocr_job.sh -l nodes=1:idc007xv5:e3-1268l-v5 -F \"results/xeon CPU FP32\" $VIDEO -N obj_det_xeon\n",
    "print(job_id_xeon[0]) \n",
    "#Progress indicators\n",
    "if job_id_xeon:\n",
    "    progressIndicator('results/xeon', 'i_progress.txt', \"Processing Time\", 0, 100)\n",
    "else:\n",
    "    print(\"Error in job submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run on Intel® Core™ CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting a job to an edge compute node with an Intel Core CPU...\")\n",
    "#Submit job to the queue\n",
    "job_id_core = !qsub ocr_job.sh -l nodes=1:idc001skl:tank-870:i5-6500te -F \"results/core CPU FP32\" $VIDEO -N obj_det_core\n",
    "print(job_id_core[0]) \n",
    "#Progress indicators\n",
    "if job_id_core:\n",
    "    progressIndicator('results/core', 'i_progress.txt', \"Processing Time\", 0, 100)\n",
    "else:\n",
    "    print(\"Error in job submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run on GPU with Intel® Core™ CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting a job to an edge compute node with an Intel GPU...\")\n",
    "#Submit job to the queue\n",
    "job_id_gpu = !qsub ocr_job.sh -l nodes=1:idc001skl:intel-hd-530 -F \"results/gpu GPU FP32\" $VIDEO -N obj_det_gpu\n",
    "print(job_id_gpu[0]) \n",
    "#Progress indicators\n",
    "if job_id_gpu:\n",
    "    progressIndicator('results/gpu', 'i_progress.txt', \"Processing Time\", 0, 100)\n",
    "else:\n",
    "    print(\"Error in job submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Check if the jobs are done\n",
    "\n",
    "To check on the jobs that were submitted, use the `qstat` command.\n",
    "\n",
    "We have created a custom Jupyter widget  to get live qstat update.\n",
    "Run the following cell to bring it up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveQstat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the jobs you have submitted (referenced by `Job ID` that gets displayed right after you submit the job in step 2.3).\n",
    "There should also be an extra job in the queue \"jupyterhub\": this job runs your current Jupyter Notebook session.\n",
    "\n",
    "The 'S' column shows the current status. \n",
    "- If it is in Q state, it is in the queue waiting for available resources. \n",
    "- If it is in R state, it is running. \n",
    "- If the job is no longer listed, it means it is completed.\n",
    "\n",
    "**Note**: Time spent in the queue depends on the number of users accessing the edge nodes. Once these jobs begin to run, they should take from 1 to 5 minutes to complete. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 View Results\n",
    "\n",
    "Once the jobs are completed, the queue system outputs the stdout and stderr streams of each job into files with names of the form\n",
    "\n",
    "`obj_det_{type}.o{JobID}`\n",
    "\n",
    "`obj_det_{type}.e{JobID}`\n",
    "\n",
    "(here, obj_det_{type} corresponds to the `-N` option of qsub).\n",
    "\n",
    "However, for this case, we may be more interested in the output result, which can be found inside the results/core/result.txt file.\n",
    "Run the cells below to display them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/xeon/result.txt\") as f: # The with keyword automatically closes the file when you are done\n",
    "...     print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/core/result.txt\") as f: # The with keyword automatically closes the file when you are done\n",
    "...     print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/gpu/result.txt\") as f: # The with keyword automatically closes the file when you are done\n",
    "...     print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, the output result you should have got is: **industries**, matching the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_list = [('core', 'Intel Core\\ni5-6500TE\\nCPU'),\n",
    "             ('gpu', 'Intel Core\\ni5-6500TE\\nGPU '),\n",
    "             ('xeon', 'Intel Xeon\\nE3-1268L v5\\nCPU'),\n",
    "             ]\n",
    "\n",
    "stats_list = []\n",
    "for arch, a_name in arch_list:\n",
    "    if 'job_id_'+arch in vars():\n",
    "        stats_list.append(('results/{dir}/stats.txt'.format(dir=arch), a_name))\n",
    "    else:\n",
    "        stats_list.append(('placeholder'+arch, a_name))\n",
    "\n",
    "summaryPlot(stats_list, 'Architecture', 'Time, mseconds', 'Inference Engine Processing Time', 'time' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
