{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='red'>  Important: </font> Before proceeding, run the following cell to check for code updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qarpo.catalog import DemoCatalog\n",
    "import os\n",
    "status = DemoCatalog(os.getcwd(), \"Demo\").ShowRepositoryControls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MICCAI 2017 Robotic Instrument Segmentation\n",
    "\n",
    "![Robotic Instrument Challenge](./figures/segmentation.gif)\n",
    "\n",
    "\n",
    "The code here refers to the winning solution by Alexey Shvets, Alexander Rakhlin, Alexandr A. Kalinin, and Vladimir Iglovikov in the [MICCAI 2017 Robotic Instrument Segmentation Challenge](https://endovissub2017-roboticinstrumentsegmentation.grand-challenge.org/). This notebook has been modified from the original found on [GitHub](https://github.com/ternaus/robot-surgery-segmentation/blob/master/Demo.ipynb) which was provided with an [MIT license](https://github.com/ternaus/robot-surgery-segmentation/blob/master/LICENSE). The data files necessary to run this notebook are included in `/data/robotic-surgery-segmentation`.\n",
    "\n",
    "![TernausNet](./figures/TernausNet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "Import dependencies for the notebook by running the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from IPython.core.display import HTML\n",
    "from qarpo.demoutils import *\n",
    "from python.utils import create_script, mask_overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Inference with PyTorch (Optional)\n",
    "Run the following cells to perform inference with the PyTorch model on an [Intel Core i5-6500TE](https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-) using the code in [pytorch_infer.py](python/pytorch_infer.py). **Wait for the progress bar to complete before running the following cells to ensure inference is complete.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create script to run pytorch_infer.py\n",
    "create_script(\"generated/pytorch_infer.sh\",\n",
    "              \"python3 python/pytorch_infer.py\")\n",
    "\n",
    "# Run the script\n",
    "job_id_infer = !qsub generated/pytorch_infer.sh -l nodes=1:idc001skl:tank-870:i5-6500te -N seg_core -e logs/ -o logs/\n",
    "if job_id_infer:\n",
    "    print(job_id_infer[0])\n",
    "    progressIndicator('results/', job_id_infer[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the original image and the inference results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"generated/input.png\")\n",
    "mask = cv2.imread(\"generated/mask.png\")\n",
    "plt.figure(1, figsize=(15, 15))\n",
    "plt.subplot(121)\n",
    "plt.axis('off')\n",
    "plt.title(\"Input Image\")\n",
    "plt.imshow(image)\n",
    "plt.subplot(122)\n",
    "plt.axis('off')\n",
    "plt.title(\"Segmentation\")\n",
    "plt.imshow(mask_overlay(image, mask));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Converting PyTorch to ONNX\n",
    "\n",
    "The ONNX models need to be generated from the original PyTorch models to be used with OpenVINO, do this by running [pytorch_to_onnx.py](python/pytorch_to_onnx.py). \n",
    "\n",
    "**Wait for the progress bar to complete before running the following cells to ensure all ONNX models have been generated.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create script to run pytorch_to_onnx.py\n",
    "create_script(\"generated/pytorch_to_onnx.sh\",\n",
    "              \"python3 python/pytorch_to_onnx.py\")\n",
    "\n",
    "# Run the script\n",
    "job_id_onnx = !qsub generated/pytorch_to_onnx.sh -l nodes=1:idc001skl:tank-870:i5-6500te -N seg_core -e logs/ -o logs/\n",
    "if job_id_onnx:\n",
    "    print(job_id_onnx[0])\n",
    "    progressIndicator('results/', job_id_onnx[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Converting ONNX Model to IR \n",
    "\n",
    "We will convert pre-trained ONNX model into an intermediate representation (IR) which will be used by OpenVINO Inference Engine in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to create binary segmentation model files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_onnx.py \\\n",
    "    --input_model \"models/onnx/surgical_tools.onnx\" \\\n",
    "    --output_dir models/ov/FP16/ \\\n",
    "    --data_type FP16 \\\n",
    "    --move_to_preprocess \\\n",
    "    --scale_values \"[0.229, 0.224, 0.225]\" \\\n",
    "    --mean_values \"[0.485, 0.456, 0.406]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to create parts segmentation model files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_onnx.py \\\n",
    "    --input_model \"models/onnx/surgical_tools_parts.onnx\" \\\n",
    "    --output_dir models/ov/FP16/ \\\n",
    "    --data_type FP16 \\\n",
    "    --move_to_preprocess \\\n",
    "    --scale_values \"[0.229, 0.224, 0.225]\" \\\n",
    "    --mean_values \"[0.485, 0.456, 0.406]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference on the edge\n",
    "\n",
    "All the code up to this point has been run within the Jupyter Notebook instance running on a development node based on an Intel Xeon Scalable processor, where the Notebook is allocated a single core. We will run the workload on other edge compute nodes represented in the IoT DevCloud. We will send work to the edge compute nodes by submitting the corresponding non-interactive jobs into a queue. For each job, we will specify the type of the edge compute server that must be allocated for the job.\n",
    "\n",
    "The job file is written in Bash, and will be executed directly on the edge compute node. For this example, we have written the job file for you in the notebook. It performs the classification using the script \"segmentation.sh\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile generated/segmentation.sh\n",
    "\n",
    "# The default path for the job is your home directory, so we change directory to where the files are.\n",
    "cd $PBS_O_WORKDIR\n",
    "\n",
    "\n",
    "OUTPUT_FILE=$1\n",
    "DEVICE=$2\n",
    "FP_MODEL=$3\n",
    "INPUT_FILE=$4\n",
    "\n",
    "if [ \"$DEVICE\" = \"HETERO:FPGA,CPU\" ]; then\n",
    "    # Environment variables and compilation for edge compute nodes with FPGAs - Updated for OpenVINO 2020.1\n",
    "    source /opt/altera/aocl-pro-rte/aclrte-linux64/init_opencl.sh\n",
    "    export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/opt/intel/openvino/bitstreams/a10_vision_design_sg1_bitstreams/BSP/a10_1150_sg1/linux64/lib\n",
    "    aocl program acl0 /opt/intel/openvino/bitstreams/a10_vision_design_sg1_bitstreams/2019R4_PL1_FP11_ResNet_VGG.aocx\n",
    "    export CL_CONTEXT_COMPILER_MODE_INTELFPGA=3\n",
    "fi\n",
    "\n",
    "python3 python/segmentation_parts.py  -m ${FP_MODEL} \\\n",
    "                               -i data/${INPUT_FILE} \\\n",
    "                               -d ${DEVICE} \\\n",
    "                               -o ${OUTPUT_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 How jobs are submitted into the queue\n",
    "\n",
    "Now that we have the job script, we can submit the jobs to edge compute nodes. In the IoT DevCloud, you can do this using the `qsub` command.\n",
    "We can submit the job to 5 different types of edge compute nodes simultaneously or just one node at at time.\n",
    "\n",
    "There are five options of `qsub` command that we use for this:\n",
    "- `-l` : this option lets us select the number and the type of nodes using `nodes={node_count}:{property}`. \n",
    "- `-F` : this option lets us send arguments to the bash script. \n",
    "- `-N` : this option lets us name the job so that it is easier to distinguish between them.\n",
    "- `-o` : this option lets us determine the path to be used for the standard output stream.\n",
    "- `-e` : this option lets us determine the path to be used for the standard error stream.\n",
    "\n",
    "\n",
    "The `-F` flag is used to pass in arguments to the job script.\n",
    "The [segmentation.sh](segmentation.sh) script takes in 4 arguments:\n",
    "1. the path to the directory for the output video and performance stats\n",
    "2. targeted device (e.g. CPU, GPU, MYRIAD, HDDL or HETERO:FPGA,CPU)\n",
    "3. the floating precision to use for inference\n",
    "4. the path to the input video\n",
    "\n",
    "The job scheduler will use the contents of `-F` flag as the argument to the job script.\n",
    "\n",
    "If you are curious to see the available types of nodes on the IoT DevCloud, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pbsnodes | grep compnode | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the properties describe the node, and number on the left is the number of available nodes of that architecture.\n",
    "\n",
    "### 4.2 Job queue submission\n",
    "\n",
    "The output of the cell is the `JobID` of your job, which you can use to track progress of a job.\n",
    "\n",
    "**Note** You can submit all the jobs at once or follow one at a time. \n",
    "\n",
    "After submission, they will go into a queue and run as soon as the requested compute resources become available. \n",
    "(tip: **shift+enter** will run the cell and automatically move you to the next cell. So you can hit **shift+enter** multiple times to quickly run multiple cells).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with an Intel Core CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel \n",
    "    Core i5-6500TE</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id_core = !qsub generated/segmentation.sh -l nodes=1:idc001skl:tank-870:i5-6500te -F \"results/ CPU FP16 short_source.mp4\" -N seg_core -e logs/ -o logs/\n",
    "print(job_id_core[0]) \n",
    "#Progress indicators\n",
    "if job_id_core:\n",
    "    progressIndicator('results/', job_id_core[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with an 8th Generation Intel Core CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/8th-gen-core-dev-kit\">UP Xtreme Edge Compute Enabling Kit\n",
    "    </a> edge node with a low power <a \n",
    "    href=\"https://ark.intel.com/content/www/us/en/ark/products/193554/intel-core-i7-8665ue-processor-8m-cache-up-to-4-40-ghz.html\">Intel \n",
    "    Core i7-8865UE</a>. The inference workload will run on the CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id_core2 = !qsub generated/segmentation.sh -l nodes=1:idc014upxa10fx1:upx-edgei7 -F \"results/ CPU FP16 short_source.mp4\" -N seg_core2 -e logs/ -o logs/\n",
    "print(job_id_core2[0]) \n",
    "#Progress indicators\n",
    "if job_id_core2:\n",
    "    progressIndicator('results/', job_id_core2[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel Xeon CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88178/Intel-Xeon-Processor-E3-1268L-v5-8M-Cache-2-40-GHz-\">Intel \n",
    "    Xeon Processor E3-1268L v5</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_xeon = !qsub generated/segmentation.sh -l nodes=1:tank-870:e3-1268l-v5 -F \"results/ CPU FP16 short_source.mp4\" -N seg_xeon -e logs/ -o logs/\n",
    "print(job_id_xeon[0])\n",
    "\n",
    "#Progress indicator\n",
    "if job_id_xeon:\n",
    "    progressIndicator('results/', job_id_xeon[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with  IEI Mustang-F100-A10 (Intel® Arria® 10 FPGA)\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core i5-6500te CPU</a> . The inference workload will run on the <a href=\"https://www.ieiworld.com/mustang-f100/en/\"> IEI Mustang-F100-A10 </a> card installed in this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_fpga = !qsub generated/segmentation.sh -l nodes=1:idc003a10:iei-mustang-f100-a10 -F \"results/ HETERO:FPGA,CPU FP16 short_source.mp4\" -N seg_fpga -e logs/ -o logs/\n",
    "print(job_id_fpga[0]) \n",
    "\n",
    "#Progress indicator\n",
    "if job_id_fpga:\n",
    "    progressIndicator('results/', job_id_fpga[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate example visualizations using an Intel Core CPU\n",
    "In the cell below, we submit a job to run the script [figures.py](python/figures.py) to an [IEI Tank 870-Q170](https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core) edge node with an [Intel Core i5-6500TE](https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-). The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create script to run figures.py\n",
    "create_script(\"generated/figures.sh\",\n",
    "              \"python3 python/figures.py\")\n",
    "\n",
    "# Run the script\n",
    "job_id_figures = !qsub generated/figures.sh -l nodes=1:idc001skl:tank-870:i5-6500te -N figures -e logs/ -o logs/\n",
    "if job_id_figures:\n",
    "    print(job_id_figures[0])\n",
    "    progressIndicator('results/', job_id_figures[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Check if the jobs are done\n",
    "\n",
    "To check on the jobs that were submitted, use the `qstat` command.\n",
    "\n",
    "We have created a custom Jupyter widget  to get live qstat update.\n",
    "Run the following cell to bring it up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveQstat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the jobs you have submitted (referenced by `Job ID` that gets displayed right after you submit the job in step 2.3).\n",
    "There should also be an extra job in the queue \"jupyterhub\": this job runs your current Jupyter Notebook session.\n",
    "\n",
    "The 'S' column shows the current status. \n",
    "- If it is in Q state, it is in the queue waiting for available resources. \n",
    "- If it is in R state, it is running. \n",
    "- If the job is no longer listed, it means it is completed.\n",
    "\n",
    "**Note**: Time spent in the queue depends on the number of users accessing the edge nodes. Once these jobs begin to run, they should take from 1 to 5 minutes to complete. \n",
    "\n",
    "***Wait!***\n",
    "\n",
    "Please wait for the inference jobs complete before proceeding to the next step.\n",
    "\n",
    "### 4.4 View Results\n",
    "\n",
    "Once the jobs are completed, the stdout and stderr streams of each job are saved into the `logs/` folder.\n",
    "\n",
    "We also saved the probability output and inference time for each input image in the folder `results/` for each architecture. \n",
    "We observe the results below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result on the Intel Core CPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank (Intel Core CPU)', \n",
    "          ['results/output_'+job_id_core[0]+'.mp4'], \n",
    "          'results/stats_'+job_id_core[0]+'.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result on the 8th Generation Intel Core CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('UP Xtreme Edge Compute Enabling Kit (Intel 8th Generation Core CPU)', \n",
    "          ['results/output_'+job_id_core2[0]+'.mp4'], \n",
    "          'results/stats_'+job_id_core2[0]+'.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result on the Intel Xeon CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank Xeon (Intel Xeon CPU)',\n",
    "          ['results/output_'+job_id_xeon[0]+'.mp4'],\n",
    "          'results/stats_'+job_id_xeon[0]+'.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result on the IEI Mustang-F100-A10 (Intel® Arria® 10 FPGA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Mustang-F100-A10',\n",
    "          ['results/output_'+job_id_fpga[0]+'.mp4'],\n",
    "          'results/stats_'+job_id_fpga[0]+'.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize results on the Intel Core CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('''{img}'''.format(img=\"<img src='generated/predictions.png'>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Assess Performance\n",
    "\n",
    "The total average time of each inference task is recorded in `results/{ARCH}/statsjob_id.txt`, where the subdirectory name corresponds to the architecture of the target edge compute node. Run the cell below to plot the results of all jobs side-by-side. Lower values mean better performance. Keep in mind that some architectures are optimized for the highest performance, others for low power or other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_list = [('core', 'Intel Core\\ni5-6500TE\\nCPU'),\n",
    "             ('core2', 'Intel Core\\ni7-8865UE\\nCPU'),\n",
    "             ('xeon', 'Intel Xeon\\nE3-1268L v5\\nCPU'),\n",
    "             ('fpga', 'Intel\\nFPGA')]\n",
    "\n",
    "stats_list = []\n",
    "\n",
    "for arch, a_name in arch_list:\n",
    "    if 'job_id_'+arch in vars():\n",
    "        stats_list.append(('results/stats_'+vars()['job_id_'+arch][0]+'.txt', a_name))\n",
    "    else:\n",
    "        stats_list.append(('placeholder'+arch, a_name))\n",
    "summaryPlot(stats_list, 'Architecture', 'Time(s)', 'Inference Engine Processing Time', 'time' )\n",
    "summaryPlot(stats_list, 'Architecture', 'Frames per second', 'Inference Engine FPS', 'fps' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Citation\n",
    "    @inproceedings{shvets2018automatic,\n",
    "    title={Automatic Instrument Segmentation in Robot-Assisted Surgery using Deep Learning},\n",
    "    author={Shvets, Alexey A and Rakhlin, Alexander and Kalinin, Alexandr A and Iglovikov, Vladimir I},\n",
    "    booktitle={2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)},\n",
    "    pages={624--628},\n",
    "    year={2018}\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
