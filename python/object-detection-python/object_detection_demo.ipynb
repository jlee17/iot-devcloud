{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection Demo: Car Detection\n",
    "\n",
    "This is a sample reference implementation to showcase object detection (car in this case) with single-shot detection (SSD) and Async API.\n",
    "Async API improves the overall frame-rate of the application by not waiting for the inference to complete but continuing to do things on the host while inference accelerator is busy. \n",
    "Specifically, this code demonstrates two parallel inference requests by processing the current frame while the next input frame is being captured. This essentially hides the latency of frame capture.\n",
    "\n",
    "## Overview of how it works\n",
    "At start-up the sample application reads the equivalent of command line arguments and loads a network and image from the video input to the Inference Engine (IE) plugin. \n",
    "A job is submitted to a hardware accelerator (Intel® Core CPU, Intel® HD Graphics GPU, Intel® Core CPU, Intel® Movidius™ Neural Compute Stick, Intel® Neural Compute Stick 2, Intel® HDDL-R, and Intel® HDDL-F.\n",
    "After the inference is completed, the output videos are appropriately stored in the /results directory, which can then be viewed within the Jupyter Notebook instance.\n",
    "\n",
    "## Demonstration objectives\n",
    "* Video as input is supported using **OpenCV**\n",
    "* Inference performed on edge hardware (rather than on the development node hosting this Jupyter notebook)\n",
    "* **OpenCV** provides the bounding boxes, labels and other information\n",
    "* Visualization of the resulting bounding boxes\n",
    "* Demonstrate the Async API in action\n",
    "\n",
    "\n",
    "## Step 0: Set Up\n",
    "\n",
    "### 0.1: Import dependencies\n",
    "\n",
    "Run the below cell to import Python dependencies needed for displaying the results in this notebook\n",
    "(tip: select the cell and use **Ctrl+enter** to run the cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import sys                                     \n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path().resolve().parent.parent))\n",
    "from demoTools.demoutils import *\n",
    "from openvino.inference_engine import IECore, IENetwork\n",
    "import cv2\n",
    "# For labeling the image\n",
    "from out_process import placeBoxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with running inference on a single image to see how the Intel® Distribution of OpenVINO™ toolkit works, then we will run inference on a video stream.\n",
    "\n",
    "We will go over detecting cars with OpenVINO in several steps:\n",
    "\n",
    "1. Create an intermediate representation (IR) of the model using the Model Optimizer.\n",
    "2. Create IEPlugin for the device.\n",
    "3. Read the model's IR using IENetwork.\n",
    "4. Load the IENetwork instance into the plugin.\n",
    "5. Preprocess image and run inference on it.\n",
    "6. Create a job file to target different hardware types.\n",
    "7. Submit jobs to the queue.\n",
    "8. View the results and hardware performance comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Intermediate Representation of the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Optimizer creates the Intermediate Representation of the model which is the device-agnostic, generic optimization of the model. Caffe\\*, TensorFlow\\*, MXNet\\*, ONNX\\*, and Kaldi\\* models are supported by Model Optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the **MobileNet-SSD** model. Download the model, specifying the name and output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name mobilenet-ssd -o raw_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert this model to the intermediate representation using Model Optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
    "--input_model raw_models/public/mobilenet-ssd/mobilenet-ssd.caffemodel \\\n",
    "--data_type FP32 \\\n",
    "--output_dir models/mobilenet-ssd/FP32 \\\n",
    "--scale 256 \\\n",
    "--mean_values [127,127,127] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need the FP16 IR version of the model for the calculations on Neural Compute Stick 2 (NCS2) and Visual Processing Unit (VPU); let's create it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
    "--input_model raw_models/public/mobilenet-ssd/mobilenet-ssd.caffemodel \\\n",
    "--data_type FP16 \\\n",
    "--output_dir models/mobilenet-ssd/FP16 \\\n",
    "--scale 256 \\\n",
    "--mean_values [127,127,127] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Plugin\n",
    "\n",
    "Let's create a function to construct a plugin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPlugin(device, extension_list):\n",
    "    ie = IECore()\n",
    "\n",
    "    # Loading additional extension libraries for the CPU\n",
    "    for extension in extension_list:\n",
    "        ie.add_extension('/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_avx2.so', device)\n",
    "    \n",
    "    return ie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the IR of the Model\n",
    "\n",
    "Let's import the optimized model into our neural network using **`IENetwork`**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createNetwork(model_xml, model_bin, ie, device):\n",
    "    # Importing network weights from IR models.\n",
    "    net = IENetwork(model=model_xml, weights=model_bin)\n",
    "    \n",
    "    # Some layers in IR models may be unsupported by some plugins. \n",
    "    if device == \"CPU\":\n",
    "        supported_layers = ie.query_network(net, device)\n",
    "        not_supported_layers = [l for l in net.layers.keys() if l not in supported_layers]\n",
    "        if len(not_supported_layers) != 0:\n",
    "            print(\"Following layers are not supported by the plugin for specified device {}:\\n {}\".\n",
    "                      format(device, ', '.join(not_supported_layers)))\n",
    "            print(\"Please try to specify cpu extensions library path in sample's command line parameters \"\n",
    "                  \"using -l or --cpu_extension command line argument\")\n",
    "            return None\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Network into the Plugin\n",
    "\n",
    "Once we have the plugin and the network, we can load the network into the plugin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadNetwork(ie, net, device):\n",
    "    # Loading IR model to the plugin.\n",
    "    exec_net = ie.load_network(network=net, num_requests=2, device_name=device)\n",
    "    \n",
    "    # Getting the input and outputs of the network\n",
    "    input_blob = next(iter(net.inputs))\n",
    "    out_blob = next(iter(net.outputs))\n",
    "    return exec_net,input_blob,out_blob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need the function to load the image using OpenCV and change its shape to be compatible with our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessImage(img_path, net, input_blob):\n",
    "    # Reading the frame from a jpeg file\n",
    "    frame = cv2.imread(img_path)\n",
    "    \n",
    "    # Reshaping data\n",
    "    n, c, h, w = net.inputs[input_blob].shape\n",
    "    in_frame = cv2.resize(frame, (w, h))\n",
    "    in_frame = in_frame.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n",
    "    return in_frame.reshape((n, c, h, w)),frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference on a Single Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to run the inference workload using the plugin. We will be running in **async_mode** by using `start_async` method. With the async_mode, the inference is started in parallel on either a separate thread or device.\n",
    "In other words, `start_async` is non-blocking and the main process is free to do any additional processing needed. \n",
    "In the next section, we will see an implementation of pipelining to mask the latency of loading and modifying images.\n",
    "\n",
    "During asynchronous runs, the different images are tracked by an integer `request_id`. Because we only have one image to process, we will just use 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For labeling the image\n",
    "from out_process import placeBoxes\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Request id to keep track of\n",
    "def runInference():\n",
    "    ie = createPlugin(device='CPU', extension_list=['/data/reference-sample-data/extension/libcpu_extension.so'])\n",
    "    model_xml = \"models/mobilenet-ssd/FP32/mobilenet-ssd.xml\"\n",
    "    model_bin = \"models/mobilenet-ssd/FP32/mobilenet-ssd.bin\"\n",
    "    net = createNetwork(model_xml, model_bin, ie, device='CPU')\n",
    "    exec_net,input_blob,out_blob = loadNetwork(ie, net, device=\"CPU\")\n",
    "    in_frame,original_frame = preprocessImage('cars_1900_first_frame.jpg', net, input_blob)\n",
    "    \n",
    "    my_request_id=0\n",
    "\n",
    "    # Starting the inference in async mode, which starts the inference in parallel\n",
    "    exec_net.start_async(request_id=my_request_id, inputs={input_blob: in_frame})\n",
    "    # ... You can do additional processing or latency masking while we wait ...\n",
    "\n",
    "    # Blocking wait for a particular request_id\n",
    "    if exec_net.requests[my_request_id].wait(-1) == 0:\n",
    "        # getting the result of the network\n",
    "        res = exec_net.requests[my_request_id].outputs[out_blob]\n",
    "        \n",
    "        # Processing the output result and adding labels on the image. Implementation is not shown in the\n",
    "        #  this notebook; you can find it in object_detection_demo_ssd_async.py\n",
    "        prob_threshold = 0.5  # 50% confidence needed for \"detection\"\n",
    "        initial_w = original_frame.shape[1]\n",
    "        initial_h = original_frame.shape[0]\n",
    "        frame = placeBoxes(res, None, prob_threshold, original_frame, initial_w, initial_h, False, my_request_id, 0)\n",
    "        fig = plt.figure(dpi=300)\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), interpolation='none')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"There was an error with the request\")\n",
    "\n",
    "runInference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Video\n",
    "\n",
    "Run the following cell to create a symlink and view the input video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -sf /data/reference-sample-data/object-detection-python/cars_1900.mp4 \n",
    "videoHTML('Cars video', ['cars_1900.mp4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Video Inferencing Works\n",
    "\n",
    "Video inferencing is similar to the image inferencing, however, there are few differences. Let's review them.\n",
    "\n",
    "The following lines determine the source of the video. We will use a pre-recorded input video file in this example, we could also use a camera by setting the input argument to 'cam'.\n",
    "\n",
    "```python\n",
    "if args.input == 'cam':\n",
    "        input_stream = 0\n",
    "        out_file_name = 'cam'\n",
    "    else:\n",
    "        input_stream = args.input\n",
    "```\n",
    "\n",
    "We capture frames from the video sample using **OpenCV VideoCapture** API.\n",
    "\n",
    "```python\n",
    "cap = cv2.VideoCapture(input_stream)\n",
    "```\n",
    "\n",
    "Finally, we have a latency masking scheme, where we post-process a frames while other frames are being processed on the inference engine. User can control the number of inference requests running in parallel.\n",
    "\n",
    "```python\n",
    "current_inference = 0\n",
    "required_inference_requests_were_executed = False\n",
    "previous_inference = 1 - args.number_infer_requests\n",
    "step = 0\n",
    "steps_count = args.number_infer_requests - 1\n",
    "\n",
    "while not required_inference_requests_were_executed or step < steps_count or cap.isOpened():\n",
    "    # ... load the next frame from cap ...\n",
    "\n",
    "    # start the next frame\n",
    "    exec_net.start_async(request_id=current_inference, inputs={input_blob: in_frame})\n",
    "\n",
    "    # see if the current frame is ready\n",
    "    if previous_inference >= 0:\n",
    "        status = infer_requests[previous_inference].wait()\n",
    "    # ... post-processing current frame ...\n",
    "    \n",
    "    # manage request ids\n",
    "    current_inference += 1\n",
    "    if current_inference >= args.number_infer_requests:\n",
    "        current_inference = 0\n",
    "        required_inference_requests_were_executed = True\n",
    "\n",
    "    previous_inference += 1\n",
    "    if previous_inference >= args.number_infer_requests:\n",
    "        previous_inference = 0\n",
    "\n",
    "    step += 1\n",
    "```\n",
    "\n",
    "The Python code takes in command line arguments for video, model etc.\n",
    "\n",
    "**Command line arguments options and how they are interpreted in the application source code**\n",
    "\n",
    "```\n",
    "SAMPLEPATH=\"/data/reference-sample-data\"\n",
    "python3 object_detection_demo_ssd_async.py -m ${SAMPLEPATH}/models/mobilenet-ssd/$3/mobilenet-ssd.xml \\\n",
    "                                           -i $INPUT_FILE \\\n",
    "                                           -o $RESULTS_PATH \\\n",
    "                                           -d $DEVICE \\\n",
    "                                           -nireq $NUM_INFER_REQS \\\n",
    "                                           -ce ${SAMPLEPATH}/extension/libcpu_extension.so\n",
    "```\n",
    "\n",
    "##### The description of the arguments used in the argument parser is the command line executable equivalent.\n",
    "* -m location of the **mobilenet-ssd** pre-trained model which has been pre-processed using the **model optimizer**\n",
    "   There is automated support built in this argument to support both FP32 and FP16 models targeting different hardware\n",
    "   (**Note** we are using mobilenet-ssd in this example. However, OpenVINO's Inference Engine is compatible with other neural network architectures such as AlexNet*, GoogleNet*, SqueezeNet* etc.,)    \n",
    "\n",
    "* -i location of the input video stream (video/cars_1900.mp4)\n",
    "* -o location where the output file with inference needs to be stored. (results/core or results/xeon or results/gpu)\n",
    "* -d Type of Hardware Acceleration (CPU or GPU or MYRIAD or HDDL or FPGA)\n",
    "* -nireq Number of inference requests running in parallel\n",
    "* -ce Absolute path to the shared library and is currently optimized for core/xeon (extension/libcpu_extension.so)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Job File\n",
    "\n",
    "All the code up to this point has been run within the Jupyter Notebook instance running on a development node based on an Intel® Xeon® Scalable Processor, where the Notebook is allocated a single core. To run inference on the entire video, we need more compute power. We will run the workload on several DevCloud's edge compute nodes. We will send work to the edge compute nodes by submitting jobs into a queue. For each job, we will specify the type of the edge compute server that must be allocated for the job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pass the specific variables to the Python code, we will use following arguments:\n",
    "\n",
    "* `-m`&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;location of the optimized **MobileNet-SSD** model's XML\n",
    "* `-i`&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;location of the input video\n",
    "* `-o`&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output directory\n",
    "* `-d`&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;hardware device type (CPU, GPU, MYRIAD, HDDL or HETERO:FPGA,CPU)\n",
    "* `-l`&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;path to the CPU extension library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job file will be executed directly on the edge compute node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile object_detection_job.sh\n",
    "\n",
    "ME=`basename $0`\n",
    "\n",
    "# The default path for the job is your home directory, so we change directory to where the files are.\n",
    "cd $PBS_O_WORKDIR\n",
    "\n",
    "# Object detection script writes output to a file inside a directory. We make sure that this directory exists.\n",
    "# The output directory is the first argument of the bash script\n",
    "while getopts 'd:f:i:r:n:?' OPTION; do\n",
    "    case \"$OPTION\" in\n",
    "    d)\n",
    "        DEVICE=$OPTARG\n",
    "        echo \"$ME is using device $OPTARG\"\n",
    "      ;;\n",
    "\n",
    "    f)\n",
    "        FP_MODEL=$OPTARG\n",
    "        echo \"$ME is using floating point model $OPTARG\"\n",
    "      ;;\n",
    "\n",
    "    i)\n",
    "        INPUT_FILE=$OPTARG\n",
    "        echo \"$ME is using input file $OPTARG\"\n",
    "      ;;\n",
    "    r)\n",
    "        RESULTS_BASE=$OPTARG\n",
    "        echo \"$ME is using results base $OPTARG\"\n",
    "      ;;\n",
    "    n)\n",
    "        NUM_INFER_REQS=$OPTARG\n",
    "        echo \"$ME is running $OPTARG inference requests\"\n",
    "      ;;\n",
    "    esac  \n",
    "done\n",
    "\n",
    "NN_MODEL=\"mobilenet-ssd.xml\"\n",
    "RESULTS_PATH=\"${RESULTS_BASE}\"\n",
    "mkdir -p $RESULTS_PATH\n",
    "echo \"$ME is using results path $RESULTS_PATH\"\n",
    "\n",
    "if [ \"$DEVICE\" = \"HETERO:FPGA,CPU\" ]; then\n",
    "    # Environment variables and compilation for edge compute nodes with FPGAs\n",
    "    # export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/opt/altera/aocl-pro-rte/aclrte-linux64/\n",
    "    # Environment variables and compilation for edge compute nodes with FPGAs - Updated for 2019-R3\n",
    "    source /opt/intel/init_openvino.sh\n",
    "    aocl program acl0 /opt/intel/openvino/bitstreams/a10_vision_design_sg1_bitstreams/2019R3_PV_PL1_FP11_ResNet_VGG.aocx\n",
    "fi\n",
    "    \n",
    "# Running the object detection code\n",
    "SAMPLEPATH=$PBS_O_WORKDIR\n",
    "python3 object_detection_demo_ssd_async.py  -m ${SAMPLEPATH}/models/mobilenet-ssd/${FP_MODEL}/${NN_MODEL} \\\n",
    "                                            -i $INPUT_FILE \\\n",
    "                                            -o $RESULTS_PATH \\\n",
    "                                            -d $DEVICE \\\n",
    "                                            -nireq $NUM_INFER_REQS \\\n",
    "                                            -ce /opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_avx2.so\n",
    "\n",
    "\n",
    "g++ -std=c++14 ROI_writer.cpp -o ROI_writer  -lopencv_core -lopencv_videoio -lopencv_imgproc -lopencv_highgui  -fopenmp -I/opt/intel/openvino/opencv/include/ -L/opt/intel/openvino/opencv/lib/\n",
    "# Rendering the output video\n",
    "SKIPFRAME=1\n",
    "RESOLUTION=0.5\n",
    "./ROI_writer $INPUT_FILE $RESULTS_PATH $SKIPFRAME $RESOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Change of Video Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video rendering is a separate task invoked by `ROI_writer` at the end of the job. To reduce rendering time, you can reduce the output video quality using the SKIP_FRAME and RESOLUTION variables. \n",
    "\n",
    "`SKIP_FRAME=1` writes all processed video frames with bounding boxes into the output video. This is the slowest option and it preserves all inference data in the output video stream. `SKIP_FRAME=2` writes every other frame of the processed frames into the output video.\n",
    "\n",
    "`RESOLUTION=1` produces the output video with the same resolution as the input video; this is the slowest option.\n",
    "`RESOLUTION<1` reduces the output video resolution. Here we have `RESOLUTION=0.5` which sets the output video resolution in each dimension to 50% of the input video's resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pbsnodes | grep compnode | awk '{print $3}' | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the properties describe the node, and number on the left is the number of available nodes of that architecture.\n",
    "\n",
    "### 2.3 Job queue submission\n",
    "\n",
    "Each cell below will submit a job to different edge compute nodes.\n",
    "The output of the cell is the `JobID` of your job, which you can use to track progress of a job.\n",
    "\n",
    "**Note** You can submit all 5 jobs at once or follow one at a time. \n",
    "\n",
    "After submission, they will go into a queue and run as soon as the requested compute resources become available. \n",
    "(tip: **shift+enter** will run the cell and automatically move you to the next cell. So you can hit **shift+enter** multiple times to quickly run multiple cells)\n",
    "\n",
    "**Note** If you want to use your own video, Change the environment variable 'VIDEO' in the following cell from \"/data/reference-sample-data/safety-gear-detection/Safety_Full_Hat_and_Vest.mp4\" to the full path of your uploaded video.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"VIDEO\"] = \"/data/reference-sample-data/object-detection-python/cars_1900.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intel® CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel \n",
    "    Core i5-6500TE</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_core = !qsub object_detection_job.sh -l nodes=1:idc001skl:tank-870:i5-6500te -F \"-r results/ -d CPU -f FP32 -i $VIDEO -n 2\" -N obj_det_core\n",
    "print(job_id_core[0]) \n",
    "#Progress indicators\n",
    "if job_id_core:\n",
    "    progressIndicator('results/', 'pre_progress_{}.txt'.format(job_id_core[0]), \"Preprocessing\", 0, 100)\n",
    "    progressIndicator('results/', 'i_progress_{}.txt'.format(job_id_core[0]), \"Inference\", 0, 100)\n",
    "    progressIndicator('results/', 'post_progress_{}.txt'.format(job_id_core[0]), \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intel® Xeon® CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88178/Intel-Xeon-Processor-E3-1268L-v5-8M-Cache-2-40-GHz-\">Intel \n",
    "    Xeon Processor E3-1268L v5</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_xeon = !qsub object_detection_job.sh -l nodes=1:idc007xv5:e3-1268l-v5 -F \"-r results/ -d CPU -f FP32 -i $VIDEO -n 2\" -N obj_det_xeon \n",
    "print(job_id_xeon[0]) \n",
    "#Progress indicators\n",
    "if job_id_xeon:\n",
    "    progressIndicator('results/', 'pre_progress_{}.txt'.format(job_id_xeon[0]), \"Preprocessing\", 0, 100)\n",
    "    progressIndicator('results/', 'i_progress_{}.txt'.format(job_id_xeon[0]), \"Inference\", 0, 100)\n",
    "    progressIndicator('results/', 'post_progress_{}.txt'.format(job_id_xeon[0]), \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intel® Core CPU with Intel® GPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core i5-6500TE</a>. The inference workload will run on the Intel® HD Graphics 530 card integrated with the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_gpu = !qsub object_detection_job.sh -l nodes=1:idc001skl:intel-hd-530 -F \"-r results/ -d GPU -f FP32 -i $VIDEO -n 4\" -N obj_det_gpu \n",
    "print(job_id_gpu[0]) \n",
    "#Progress indicators\n",
    "if job_id_gpu:\n",
    "    progressIndicator('results/', 'pre_progress_{}.txt'.format(job_id_gpu[0]), \"Preprocessing\", 0, 100)\n",
    "    progressIndicator('results/', 'i_progress_{}.txt'.format(job_id_gpu[0]), \"Inference\", 0, 100)\n",
    "    progressIndicator('results/', 'post_progress_{}.txt'.format(job_id_gpu[0]), \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intel® Arria® 10 FPGA\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core i5-6500te CPU</a> . The inference workload will run on the <a href=\"https://www.ieiworld.com/mustang-f100/en/\"> IEI Mustang-F100-A10 </a> card installed in this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_fpga = !qsub object_detection_job.sh -l nodes=1:idc003a10:iei-mustang-f100-a10 -F \"-r results/ -d HETERO:FPGA,CPU -f FP32 -i $VIDEO -n 4\" -N obj_det_fpga\n",
    "\n",
    "print(job_id_fpga[0]) \n",
    "#Progress indicators\n",
    "if job_id_fpga:\n",
    "    progressIndicator('results/', 'pre_progress_{}.txt'.format(job_id_fpga[0]), \"Preprocessing\", 0, 100)\n",
    "    progressIndicator('results/', 'i_progress_{}.txt'.format(job_id_fpga[0]), \"Inference\", 0, 100)\n",
    "    progressIndicator('results/', 'post_progress_{}.txt'.format(job_id_fpga[0]), \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intel® Neural Compute Stick 2\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core i5-6500te CPU</a>. The inference workload will run on an <a \n",
    "    href=\"https://software.intel.com/en-us/neural-compute-stick\">Intel Neural Compute Stick 2</a> installed in this  node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_ncs2 = !qsub object_detection_job.sh -l nodes=1:idc004nc2:intel-ncs2 -F \"-r results/ -d MYRIAD -f FP16 -i $VIDEO -n 4\" -N obj_det_ncs2\n",
    "print(job_id_ncs2[0]) \n",
    "#Progress indicators\n",
    "if job_id_ncs2:\n",
    "    progressIndicator('results/', 'pre_progress_{}.txt'.format(job_id_ncs2[0]), \"Preprocessing\", 0, 100)\n",
    "    progressIndicator('results/', 'i_progress_{}.txt'.format(job_id_ncs2[0]), \"Inference\", 0, 100)\n",
    "    progressIndicator('results/', 'post_progress_{}.txt'.format(job_id_ncs2[0]), \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IEI Mustang-V100-MX8 ( Intel® Movidius™ Myriad™ X Vision Processing Unit (VPU))\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core i5-6500te CPU</a>. The inference workload will run on an <a \n",
    "    href=\"https://www.ieiworld.com/mustang-v100/en/\">IEI Mustang-V100-MX8 </a>accelerator installed in this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_hddlr = !qsub object_detection_job.sh -l nodes=1:idc002mx8:iei-mustang-v100-mx8 -F \"-r results/ -d HDDL -f FP16 -i $VIDEO -n 128\" -N obj_det_hddlr\n",
    "print(job_id_hddlr[0]) \n",
    "#Progress indicators\n",
    "if job_id_hddlr:\n",
    "    progressIndicator('results/', 'pre_progress_{}.txt'.format(job_id_hddlr[0]), \"Preprocessing\", 0, 100)\n",
    "    progressIndicator('results/', 'i_progress_{}.txt'.format(job_id_hddlr[0]), \"Inference\", 0, 100)\n",
    "    progressIndicator('results/', 'post_progress_{}.txt'.format(job_id_hddlr[0]), \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UP Squared Grove IoT Development Kit\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/up-squared-grove-dev-kit\">UP Squared Grove IoT Development Kit</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/96488/Intel-Atom-x7-E3950-Processor-2M-Cache-up-to-2-00-GHz-\">Intel Atom® x7-E3950 Processor</a>. The inference  workload will run on the integrated Intel® HD Graphics 505 card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_up2 = !qsub object_detection_job.sh -l nodes=1:up-squared -F \"-r results/ -d GPU -f FP32 -i $VIDEO -n 2\" -N obj_det_up2\n",
    "print(job_id_up2[0]) \n",
    "#Progress indicators\n",
    "if job_id_up2:\n",
    "    progressIndicator('results/', 'pre_progress_{}.txt'.format(job_id_up2[0]), \"Preprocessing\", 0, 100)\n",
    "    progressIndicator('results/', 'i_progress_{}.txt'.format(job_id_up2[0]), \"Inference\", 0, 100)\n",
    "    progressIndicator('results/', 'post_progress_{}.txt'.format(job_id_up2[0]), \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Progress\n",
    "\n",
    "Check the progress of the jobs. `Q` status stands for `queued`, `R` for `running`. How long a job is being queued is dependent on number of the users. It should take up to 5 minutes for a job to run. If the job is no longer listed, it's done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveQstat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the jobs you have submitted (referenced by `Job ID` that gets displayed right after you submit the job in step 2.3).\n",
    "There should also be an extra job in the queue \"jupyterhub\": this job runs your current Jupyter Notebook session.\n",
    "\n",
    "The 'S' column shows the current status. \n",
    "- If it is in Q state, it is in the queue waiting for available resources. \n",
    "- If it is in R state, it is running. \n",
    "- If the job is no longer listed, it means it is completed.\n",
    "\n",
    "**Note**: Time spent in the queue depends on the number of users accessing the edge nodes. Once these jobs begin to run, they should take from 1 to 5 minutes to complete. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Wait!***\n",
    "\n",
    "Please wait for the inference jobs and video rendering complete before proceeding to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## While you wait...\n",
    "\n",
    "### Why do we have three progress indicators for every job? \n",
    "\n",
    "In a real-life inference application, the video processing pipeline may have these phases:\n",
    "\n",
    "1. Preprocessing (such as video decoding, frame capture from camera, or fetching a frame from a network, resizing, cropping, contrast normalization, etc.) The performance of this phase will often depend on the CPU, storage, and networking speed.\n",
    "2. Inference (running a neural network forward propagation procedure). The performance of inference depends mostly on the compute device (CPU or an accelerator).\n",
    "3. Postprocessing (feeding data into the next pipeline stage, writing to file, sending it to a database, or even encoding a new video for a human viewer). This phase will often use the CPU, storage, or network, and will not use an accelerator.\n",
    "\n",
    "In our demonstration samples, we separate these three phases into separate workloads. This separation allows you to independently judge the relative impact of each phase and draw conclusions for your applications. \n",
    "\n",
    "### What does it mean for my application?\n",
    "\n",
    "Your application may have more or less pre/post-processing work depending on how you get your data (from storage, sensor or network), what is the resolution of your raw data, is it encoded or not, whether you need to produce an output video, etc. Therefore, you need to understand how your processing pipeline relates to this demo before drawing conclusions on performance.\n",
    "\n",
    "In this demo, we provide three performance numbers for each architecture. However, they are not hard limits on the solution's performance. You need to relate these numbers and code to what you are doing in your own project.\n",
    "\n",
    "### The total time is not necessarily equal to the sum of the parts\n",
    "\n",
    "It is important to realize that in a real-life application, you may want to fuse preprocessing, inference, and postprocessing, as opposed to separating them as we did here. Fused preprocessing has several advantages:\n",
    "* If inference is run asynchronously on the accelerator, the rest of the system is free for preprocessing future frames or postprocessing past frames. This allows you to mask the pre/post processing time behind inference time. \n",
    "* With fused preprocessing, inference and postprocessing, your pipeline has better data locality, allowing you to reuse data in the caches, in the application's memory, or in the hard drive cache. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: View Results\n",
    "\n",
    "Once the jobs are completed, the queue system outputs the `stdout` and `stderr` streams of each job into files with names\n",
    "`obj_det_{type}.o{JobID}` and `obj_det_{type}.e{JobID}`. Here, obj_det_{type} corresponds to the `-N` option of qsub. For example, `core` for Core CPU target.\n",
    "\n",
    "You can find the output video files inside the `results` directory. We wrote a short utility script that will display these videos within the notebook. See `demoutils.py` if you are interested in understanding further how the results are displayed in notebook. \n",
    "\n",
    "`obj_det_{type}.e{JobID}`\n",
    "\n",
    "(here, obj_det_{type} corresponds to the `-N` option of qsub).\n",
    "\n",
    "However, for this case, we may be more interested in the output video files. They are stored in mp4 format inside the `results/` directory.\n",
    "We wrote a short utility script that will display these videos with in the notebook.\n",
    "Run the cells below to display them.\n",
    "See `demoutils.py` if you are interested in understanding further how the results are displayed in notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank (Intel Core CPU)', \n",
    "          ['results/output_{}.mp4'.format(job_id_core[0])], \n",
    "          'results/stats_{}.txt'.format(job_id_core[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank Xeon (Intel Xeon CPU)',\n",
    "          ['results/output_{}.mp4'.format(job_id_xeon[0])],\n",
    "          'results/stats_{}.txt'.format(job_id_xeon[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Intel GPU (Intel Core + Onboard GPU)', \n",
    "          ['results/output_{}.mp4'.format(job_id_gpu[0])],\n",
    "          'results/stats_{}.txt'.format(job_id_gpu[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank + IEI Mustang-F100-A10 (Intel® Arria® 10 FPGA)',\n",
    "          ['results/output_{}.mp4'.format(job_id_fpga[0])],\n",
    "          'results/stats_{}.txt'.format(job_id_fpga[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank + Intel CPU + Intel NCS2',\n",
    "          ['results/output_{}.mp4'.format(job_id_ncs2[0])],\n",
    "          'results/stats_{}.txt'.format(job_id_ncs2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank + IEI Mustang-V100-MX8 ( Intel® Movidius™ Myriad™ X Vision Processing Unit (VPU))',\n",
    "          ['results/output_{}.mp4'.format(job_id_hddlr[0])],\n",
    "          'results/stats_{}.txt'.format(job_id_hddlr[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('UP Squared Grove IoT Development Kit (UP2)',\n",
    "          ['results/output_{}.mp4'.format(job_id_up2[0])],\n",
    "          'results/stats_{}.txt'.format(job_id_up2[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "\n",
    "The running time of each inference task is recorded in `stats_*job_id*_*architectute*.txt` in `results` folder, where the *architecture* corresponds to the architecture of the target edge compute node. Run the cell below to plot the results of all jobs side-by-side. Lower values for processing time mean better performance. Keep in mind that some architectures are optimized for the highest performance, others for low power or other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_list = [('core', 'Intel Core\\ni5-6500TE\\nCPU'),\n",
    "             ('xeon', 'Intel Xeon\\nE3-1268L v5\\nCPU'),\n",
    "             ('gpu', ' Intel Core\\ni5-6500TE\\nGPU'),\n",
    "             ('fpga', ' IEI Mustang\\nF100-A10\\nFPGA'),\n",
    "             ('hddlr', ' IEI Mustang\\nV100-MX8\\nVPU'),\n",
    "             ('ncs2', 'Intel\\nNCS2'),\n",
    "             ('up2', 'Intel Atom\\nx7-E3950\\nUP2/GPU')]\n",
    "\n",
    "stats_list = []\n",
    "for arch, a_name in arch_list:\n",
    "    if 'job_id_'+arch in vars():\n",
    "        stats_list.append(('results/stats_'+vars()['job_id_'+arch][0]+'.txt', a_name))\n",
    "    else:\n",
    "        stats_list.append(('placeholder'+arch, a_name))\n",
    "\n",
    "summaryPlot(stats_list, 'Architecture', 'Time, seconds', 'Inference Engine Processing Time', 'time' )\n",
    "summaryPlot(stats_list, 'Architecture', 'Frames per second', 'Inference Engine FPS', 'fps' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
