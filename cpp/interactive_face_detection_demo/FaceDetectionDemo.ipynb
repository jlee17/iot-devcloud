{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Face Detection Sample\n",
    "\n",
    "This sample showcases Object Detection task applied for face recognition using sequence of neural networks.\n",
    "Async API usage can improve overall frame-rate of the application, because rather than wait for inference to complete,\n",
    "the application can continue operating on the host while accelerator is busy.\n",
    "This sample maintains three parallel infer requests for the Age/Gender Recognition, Head Pose Estimation, and Emotions Recognition that run simultaneously.\n",
    "\n",
    "Other sample objectives are:\n",
    "\n",
    "*\tVideo as input support via OpenCV\n",
    "*\tVisualization of the resulting face bounding boxes from Face Detection network\n",
    "*\tVisualization of age/gender, head pose and emotion information for each detected face\n",
    "\n",
    "OpenCV is used to draw resulting bounding boxes, labels, and other information. You can copy and paste this code without pulling Inference Engine sample helpers into your application\n",
    "\n",
    "## How it Works\n",
    "\n",
    "*\tThe application reads command line parameters and loads up to four networks depending on `-m...` options family to the Inference Engine.\n",
    "*\tThe application gets a frame from the OpenCV's VideoCapture.\n",
    "*\tThe application performs inference on the frame detection network.\n",
    "*\tThe application performs three simultaneous inferences, using the Age/Gender, Head Pose and Emotions detection networks if they are specified in command line.\n",
    "*\tThe application displays the results.\n",
    "\n",
    "The new Async API operates with a new notion of the Infer Request that encapsulates the inputs/outputs and separates scheduling and waiting for result. For more information about Async API and the difference between Sync and Async modes performance, refer to **How it Works** and **Async API** sections in [Object Detection SSD, Async API Performance Showcase Sample](@ref InferenceEngineObjectDetectionSSDDemoAsyncApplication).\n",
    "\n",
    "\n",
    "## Compiling\n",
    "\n",
    "There are two source files: \n",
    "* [detectors.cpp](detectors.cpp) -- contains the various helper functions\n",
    "* [main.cpp](main.cpp) -- contains the main loop\n",
    "\n",
    "We have provided a Makefile for compiling the application. Run the following cell to run the make command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path().resolve().parent.parent))\n",
    "from demoTools.demoutils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source /opt/intel/openvino/bin/setupvars.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces an executable [interactive_face_detection_demo](). This executable takes in a number of different command line arguments.\n",
    "\n",
    "Run the following cell to see the list: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "./interactive_face_detection_demo -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The version of the cpp file here is a slightly modified version of the faca_detector code built-in to the the Intel® Distribution of OpenVINO™ toolkit.\n",
    "In this version, the result is written into a output mp4 file specified by the `-o` flag. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading IR Model\n",
    "\n",
    "The Intel® Distribution of OpenVINO™ toolkit comes with model downloader that can download pre-compiled Intermediate Representation (IR) models that are optimized for different end-point target devices.\n",
    "These models can be created from existing DNN models from popular frameworks (e.g. Caffe*, TF) using the Model Optimizer. \n",
    "\n",
    "Run the following cell to see the models available through `model_downloader.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --print_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** the '!' is a special Jupyter Notebook command that allows you to run shell commands as if you are in a commannd line. So the above command will work straight out of the box on in a terminal (with '!' removed).\n",
    "\n",
    "In this demo we will be using the 4 different types of models, with two model files each for FP16 and FP32. These models can be downloaded with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name face-detection-adas-0001 -o models\n",
    "!/opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name head-pose-estimation-adas-0001 -o models\n",
    "!/opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name age-gender-recognition-retail-0013 -o models\n",
    "!/opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name emotions-recognition-retail-0003 -o models\n",
    "!/opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name facial-landmarks-35-adas-0002 -o models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input arguments are as follows:\n",
    "* --name : name of the model you want to download. It should be one of the models listed in the previous cell\n",
    "* -o : output directory. If this directory does not exist, it will be created for you.\n",
    "\n",
    "There are more arguments to this script and you can get the full list using the `-h` option.\n",
    "\n",
    "\n",
    "### Running the inference\n",
    "\n",
    "Now we are ready to run the inference workload. In this step we will be submitting the workload as a job to the job queue.\n",
    "\n",
    "Currently, you are on what is called a \"devnode\". On this system, you are allocated just one core on a large Intel® Xeon® CPU. The purpose of this node is to develop code on the devnode and run minimal sections of Jupyter* Notebooks, but it is not meant for compute intensive jobs like deep learning inference. So we need to request additional resources from the cluster of Edge nodes to run the inference, and this is done through the job queue.\n",
    "\n",
    "To put an item on the job queue, we must first create a bash script that run the workload we want. Run the following cell to create bash script [face_detection_demo.sh](face_detection_demo.sh) which will be our job script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile face_detection_demo.sh\n",
    "#PBS\n",
    "INPUT_FILE=$1\n",
    "DEVICE=$2\n",
    "FP_MODEL=$3\n",
    "OUTPUT_FILE=$4\n",
    "\n",
    "mkdir -p $4\n",
    "\n",
    "if [ \"$DEVICE\" = \"HETERO:FPGA,CPU\" ]; then\n",
    "    # Environment variables and compilation for edge compute nodes with FPGAs\n",
    "    source /opt/fpga_support_files/setup_env.sh\n",
    "    aocl program acl0 /opt/intel/openvino/bitstreams/a10_vision_design_sg1_bitstreams/2019R3_PV_PL1_FP16_MobileNet_Clamp.aocx\n",
    "fi\n",
    "\n",
    "#if [ \"$FP_MODEL\" = \"FP16\" ]; then\n",
    "#  FPEXT='-fp16'\n",
    "#fi\n",
    "\n",
    "cd $PBS_O_WORKDIR\n",
    "export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/\n",
    "    \n",
    "./interactive_face_detection_demo -i $INPUT_FILE \\\n",
    "-m models/intel/face-detection-adas-0001/$FP_MODEL/face-detection-adas-0001.xml \\\n",
    "-m_hp models/intel/head-pose-estimation-adas-0001/$FP_MODEL/head-pose-estimation-adas-0001.xml \\\n",
    "-m_ag models/intel/age-gender-recognition-retail-0013/$FP_MODEL/age-gender-recognition-retail-0013.xml \\\n",
    "-m_em models/intel/emotions-recognition-retail-0003/$FP_MODEL/emotions-recognition-retail-0003.xml \\\n",
    "-m_lm models/intel/facial-landmarks-35-adas-0002/$FP_MODEL/facial-landmarks-35-adas-0002.xml \\\n",
    "-l /opt/intel/openvino/inference_engine/lib/intel64/libcpu_extension_sse4.so \\\n",
    "-d $DEVICE \\\n",
    "-o $OUTPUT_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put this script on the job queue, we use the command `qsub`.\n",
    "There are two important arguments we use with this command.\n",
    "\n",
    "First, the `-l` flag.\n",
    "This flag is used to specify what type of resources to request from the cluster.\n",
    "For example this can be used to request an Intel® Xeon® CPU based system, or it can be used to request a system with an FPGA accelerator card in it.\n",
    "The syntax is `-l nodes=1:<tag>` where `<tag>` is the descriptor tag for the resource you want.\n",
    "For example, `-l nodes=1:tank-870:e3-1268l-v5` will request an Intel® Xeon® system.\n",
    "To see the list of available tags, and the number of avilable systems, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pbsnodes | grep compnode | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then there is the `-F` flag, which is used to pass in arguments to the job script.\n",
    "The [face_detection_demo.sh](face_detection_demo.sh) takes in 2 arguments:\n",
    "1) the path to the video to run inference on\n",
    "2) targeted device (CPU,GPU,MYRIAD)\n",
    "The job scheduler will use the contents of `-F` flag as the argument to the job script.\n",
    "\n",
    "The following line will request an Intel Xeon system, and passes in \"faces-recognition-walking.mp4 CPU\" to the job script. Run the cell to submit this job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"VIDEO\"] = \"faces-recognition-walking-and-pause.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with an Intel® Core™ CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel \n",
    "    Core i5-6500TE</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Submitting job to Intel Core CPU...\")\n",
    "#Submit job to the queue\n",
    "job_id_core = !qsub face_detection_demo.sh -l nodes=1:idc001skl:tank-870:i5-6500te -F \"$VIDEO CPU FP32 results/core\" -N face_detection_core\n",
    "print(job_id_core[0])\n",
    "\n",
    "#Progress indicators\n",
    "if job_id_core:\n",
    "    progressIndicator('results/core', 'i_progress_'+job_id_core[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel® Xeon® CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88178/Intel-Xeon-Processor-E3-1268L-v5-8M-Cache-2-40-GHz-\">Intel \n",
    "    Xeon Processor E3-1268L v5</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting job to Intel Xeon CPU...\")\n",
    "#Submit job to the queue\n",
    "job_id_xeon = !qsub face_detection_demo.sh -l nodes=1:idc007xv5:e3-1268l-v5 -F \"$VIDEO CPU FP32 results/xeon\" -N face_detection_xeon\n",
    "print(job_id_xeon[0])\n",
    "\n",
    "#Progress indicators\n",
    "if job_id_xeon:\n",
    "    progressIndicator('results/xeon', 'i_progress_'+job_id_xeon[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you should see a output like \"{job_id}.c003\", where {job_id} is a number.\n",
    "This is your job ID, and this value can be used to check on the progress of the job down the line.\n",
    "Furthermore, the above job script has been written so that it uses its job_id as the name of the output video.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One bigadvantage of the Job queue system is that you may submit multiple jobs at once. \n",
    "These jobs will be run as soon as resources are available, and may all run at once if the cluster is not busy.\n",
    "Here are few other \"preset\" jobs that for your convenience that you can run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel® Core™ CPU and using the onboard Intel GPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core i5-6500TE</a>. The inference workload will run on the Intel® HD Graphics 530 card integrated with the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting job to Intel Core CPU with Intel GPU...\")\n",
    "#Submit job to the queue\n",
    "job_id_gpu = !qsub face_detection_demo.sh -l nodes=1:idc001skl:intel-hd-530 -F \"$VIDEO GPU FP32 results/gpu\" -N face_detection_gpu\n",
    "print(job_id_gpu[0])\n",
    "\n",
    "#Progress indicators\n",
    "if job_id_gpu:\n",
    "    progressIndicator('results/gpu', 'i_progress_'+job_id_gpu[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with  IEI Mustang-F100-A10 (Intel® Arria® 10 FPGA)\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core i5-6500te CPU</a> . The inference workload will run on the <a href=\"https://www.ieiworld.com/mustang-f100/en/\"> IEI Mustang-F100-A10 </a> card installed in this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting job to node with Intel FPGA HDDL-F...\")\n",
    "#Submit job to the queue\n",
    "job_id_fpga = !qsub face_detection_demo.sh -l nodes=1:idc003a10:iei-mustang-f100-a10 -F \"$VIDEO HETERO:FPGA,CPU FP32 results/fpga\"\n",
    "print(job_id_fpga[0])\n",
    "\n",
    "#Progress indicators\n",
    "if job_id_fpga:\n",
    "    progressIndicator('results/fpga', 'i_progress_'+job_id_fpga[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel® Movidius™ Neural Compute Stick 2\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core i5-6500te CPU</a>. The inference workload will run on an <a \n",
    "    href=\"https://software.intel.com/en-us/neural-compute-stick\">Intel Neural Compute Stick 2</a> installed in this  node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting job to node with Intel NCS2...\")\n",
    "#Submit job to the queue\n",
    "job_id_ncs2 = !qsub face_detection_demo.sh -l nodes=1:idc004nc2:intel-ncs2 -F \"$VIDEO MYRIAD FP16 results/ncs2\"\n",
    "print(job_id_ncs2[0])\n",
    "\n",
    "#Progress indicators\n",
    "if job_id_ncs2:\n",
    "    progressIndicator('results/ncs2', 'i_progress_'+job_id_ncs2[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `qstat` command is used to track the progress of the jobs. \n",
    "We have provided a utility funtion \"liveQstat()\" that provide a live updating GUI for you.\n",
    "Run the following cell to check on the progress of your earlier jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveQstat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the jobs are done. The following cell can be used to display the output. \n",
    "The videoHTML is a utility function provided in [demoutils.py](demoutils.py).\n",
    "This takes one argument, which is the path to the video (see above for convention for the path name).\n",
    "\n",
    "For your convenience we have stored the jobid from the above `qsub` commands, and the following cells "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank (Intel Core CPU)', \n",
    "          ['results/core/output.mp4'],\n",
    "          'results/core/stats_'+job_id_core[0]+'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank Xeon (Intel Xeon CPU)',\n",
    "          ['results/xeon/output.mp4'],\n",
    "          'results/xeon/stats_'+job_id_xeon[0]+'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Intel GPU (Intel Core + Onboard GPU)', \n",
    "          ['results/gpu/output.mp4'],\n",
    "          'results/gpu/stats_'+job_id_gpu[0]+'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank + IEI Mustang-F100-A10 (Intel® Arria® 10 FPGA)',\n",
    "          ['results/fpga/output.mp4'],\n",
    "          'results/fpga/stats_'+job_id_fpga[0]+'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank + Intel CPU + Intel NCS2',\n",
    "          ['results/ncs2/output.mp4'],\n",
    "          'results/ncs2/stats_'+job_id_ncs2[0]+'.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "arch_list = [('core', 'Intel Core\\ni5-6500TE\\nCPU'),\n",
    "             ('xeon', 'Intel Xeon\\nE3-1268L v5\\nCPU'),\n",
    "             ('gpu', ' Intel Core\\ni5-6500TE\\nGPU'),\n",
    "             ('fpga', ' IEI Mustang\\nF100-A10\\nFPGA'),\n",
    "             ('ncs2', 'Intel\\nNCS2')]\n",
    "\n",
    "stats_list = []\n",
    "for arch, a_name in arch_list:\n",
    "    if 'job_id_'+arch in vars():\n",
    "        stats_list.append(('results/'+arch+'/stats_'+vars()['job_id_'+arch][0]+'.txt', a_name))\n",
    "    else:\n",
    "        stats_list.append(('placeholder'+arch, a_name))\n",
    "summaryPlot(stats_list, 'Architecture', 'Time, seconds', 'Inference Engine Processing Time', 'time' )\n",
    "summaryPlot(stats_list, 'Architecture', 'Frames per second', 'Inference Engine FPS', 'fps' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
